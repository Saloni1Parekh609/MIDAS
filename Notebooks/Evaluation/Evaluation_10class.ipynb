{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHBgOTI50GQ3"
   },
   "source": [
    "#Evaluation\n",
    "-------\n",
    "\n",
    "The following types of datasets can be evaluated:\n",
    "CSV file with columns | FilePath | Label |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXEKiI2j0B0s",
    "outputId": "9c64dae2-e53c-4141-9b89-32ebadca57a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QopqeptM0M3K"
   },
   "outputs": [],
   "source": [
    "!pip install pycm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_rwPNyM0cgM"
   },
   "outputs": [],
   "source": [
    "#Unzip a file if necessary\n",
    "\n",
    "# Link to the zip file: https://drive.google.com/file/d/1g4dHphWLCX1PisdXacrfw8kdn7MGoxv5/view?usp=sharing\n",
    "!cp -r \"/content/drive/MyDrive/MIDAS/Point1/train.zip\" \"/content/train.zip\" \n",
    "!unzip train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LMrqU7lw0OK2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, ZeroPad2d\n",
    "from torchvision import transforms, datasets\n",
    "import shutil\n",
    "from sklearn.metrics import auc, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d \n",
    "import random\n",
    "from pycm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVttg8An0RHX"
   },
   "source": [
    "#Data Preparation for CSVs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gZMFDeto0QSQ"
   },
   "outputs": [],
   "source": [
    "#All images are resized to 200 * 200 and have been normalized\n",
    "def _preprocess(image):\n",
    "    # Preprocessing step\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),              #Conversion to PIL Image\n",
    "        transforms.Resize((200, 200)),        #Resize image to 200 * 200\n",
    "        transforms.ToTensor(),                #Conversion to Tensor\n",
    "        transforms.Normalize((0.5, ), (0.5,)) #Normalise Image\n",
    "    ])\n",
    "    return img_transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iVyFOLjk0ldb"
   },
   "outputs": [],
   "source": [
    "class Images_test(Dataset):\n",
    "  def __init__(self, df):\n",
    "        self.data = df\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    #print(index)\n",
    "    image1 = cv.imread(self.data.iloc[index, 0], cv.IMREAD_GRAYSCALE)\n",
    "    image1 = _preprocess(image1)\n",
    "\n",
    "    label = self.data.iloc[index, 1]\n",
    "\n",
    "    return image1, torch.from_numpy(np.array([label], dtype=np.float32)), self.data.iloc[index, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "YNwBXMRC0rFQ",
    "outputId": "968423b2-2aa0-4743-9e94-8704c6429a44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FilePath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>/content/train/Sample009/img009-030.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/content/train/Sample002/img002-040.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>/content/train/Sample009/img009-013.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>/content/train/Sample001/img001-011.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>/content/train/Sample009/img009-016.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    FilePath  Label\n",
       "334  /content/train/Sample009/img009-030.png      9\n",
       "6    /content/train/Sample002/img002-040.png      2\n",
       "345  /content/train/Sample009/img009-013.png      9\n",
       "226  /content/train/Sample001/img001-011.png      1\n",
       "328  /content/train/Sample009/img009-016.png      9"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"path_to/MIDAS/Point2/one_to_ten.csv\") #Please add appropriate link\n",
    "df[\"FilePath\"] = \"path_to/MIDAS/Notebooks\" + df[\"FilePath\"]\n",
    "df = df.sample(frac = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmSmLIya0wzd",
    "outputId": "63d311fa-7c78-4417-d5de-d09505126779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape on batch size = torch.Size([1, 1, 200, 200])\n",
      "labels shape on batch size = torch.Size([1, 1])\n",
      "File path shape on batch size = ('/content/train/Sample005/img005-024.png',)\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = Images_test(df)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_iter = iter(val_loader)\n",
    "images1, labels, fpath = val_iter.next()\n",
    "print('images shape on batch size = {}'.format(images1.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))\n",
    "print('File path shape on batch size = {}'.format(fpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yig3WXFM10uI"
   },
   "source": [
    "#Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T3UQ9Hag2Zip"
   },
   "outputs": [],
   "source": [
    "class smallModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(smallModel, self).__init__()\n",
    "    \n",
    "    self.zp1 = nn.ZeroPad2d(1)\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "    self.mp1 = nn.MaxPool2d(kernel_size=2)\n",
    "    self.dp = nn.Dropout(p=0.3)\n",
    "\n",
    "    self.zp2 = nn.ZeroPad2d(1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "    self.mp2 = nn.MaxPool2d(kernel_size=2)\n",
    "    #self.dp = nn.Dropout(p=0.3)\n",
    "\n",
    "    self.zp3 = nn.ZeroPad2d(1)\n",
    "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "    self.mp3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "    self.mp4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "    self.mp5 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "    self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc1 = nn.Linear(512 * 6 * 6, 9216)\n",
    "    self.fc2 = nn.Linear(9216, 4096)\n",
    "    self.fc3 = nn.Linear(4096, 1024)\n",
    "    self.fc4 = nn.Linear(512 * 6 * 6, 10)\n",
    "\n",
    "    self.row_emb = nn.Parameter(torch.rand(64, 512 // 2))\n",
    "    self.col_emb = nn.Parameter(torch.rand(64, 512 // 2))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.dp(self.relu(self.conv1(x)))\n",
    "    x = self.mp1(x)\n",
    "\n",
    "    x = self.dp(self.relu(self.conv2(x)))\n",
    "    x = self.mp2(x)\n",
    "\n",
    "    x = self.dp(self.relu(self.conv3(x)))\n",
    "    x = self.mp3(x)\n",
    "\n",
    "    x = self.dp(self.relu(self.conv4(x)))\n",
    "    x = self.mp4(x)\n",
    "\n",
    "    x = self.dp(self.relu(self.conv5(x)))\n",
    "    x = self.mp5(x)\n",
    "\n",
    "    H = x.shape[-1]\n",
    "    W = x.shape[-2]\n",
    "    pos = torch.cat([self.col_emb[:W].unsqueeze(0).repeat(H, 1, 1), self.row_emb[:H].unsqueeze(1).repeat(1, W, 1),], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    x = x.flatten(2).permute(2, 0, 1)\n",
    "\n",
    "    x = self.transformer_encoder(pos + x)\n",
    "\n",
    "    x = x.permute(1, 2, 0)\n",
    "    x = torch.reshape(x, ((x.shape)[0], 512 * 36))\n",
    "\n",
    "    #x = self.relu(self.fc1(x))\n",
    "    #x = self.relu(self.fc2(x))\n",
    "    #x = self.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "  \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TMRGL8v00MQ",
    "outputId": "cbc99565-d162-4328-f358-40a702c662e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1597, device='cuda:0', requires_grad=True)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = smallModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay = 1e-4)\n",
    "\n",
    "#Link for model: https://drive.google.com/file/d/1--JfhXdWyAJb1PARlKY1-w8nP2tm-GbY/view?usp=sharing\n",
    "checkpoint = torch.load(\"path_to/finetuned_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch_last = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "print(loss)\n",
    "print(epoch_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CEfS8WF00yd"
   },
   "outputs": [],
   "source": [
    "#Evaluation begins\n",
    "i = 0\n",
    "l1 = []\n",
    "l2 = []\n",
    "vcorrect = 0\n",
    "\n",
    "model.eval()\n",
    "#test_loader = DataLoader(valset, batch_size=1, shuffle=True)\n",
    "for batch, (img, target, fpath) in enumerate(val_loader):\n",
    "    img, target = img.to(device=device, dtype=torch.float), target.to(device=device, dtype=torch.long)\n",
    "\n",
    "    print(batch)\n",
    "    output = model(img)\n",
    "    \n",
    "    _,pred = torch.max(output, dim=1)\n",
    "    l1.append(target.item())\n",
    "    l2.append(pred.item())\n",
    "\n",
    "df_label = pd.DataFrame(columns=[\"Target\", \"Pred\"])\n",
    "df_label[\"Target\"] = l1\n",
    "df_label[\"Pred\"] = l2\n",
    "df_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rc6Xc04M07hM"
   },
   "outputs": [],
   "source": [
    "y = np.array(df_label[\"Target\"])\n",
    "y_pred = np.array(df_label[\"Pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxHfADgR1AAC"
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSQJSZ-h-DYt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Evaluation_10class.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
