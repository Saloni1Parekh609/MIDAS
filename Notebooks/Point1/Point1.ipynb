{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Point1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN/dOdt1r819XhCvh1LVs9l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zY2SEh-q_-4_"},"source":["#Implementation for Point 1\n","------\n","\n","##Definition for Point 1 </br>\n","Use this dataset (https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip) to train a CNN. Use no other data source or pretrained networks, and explain your design choices during preprocessing, model building and training. Also, cite the sources you used to borrow techniques. A test set will be provided later to judge the performance of your classifier. Please save your model checkpoints.\n","\n","##Solution\n","The explanation for the design choices will be given in the following sections.\n","\n","###Model Architecture\n","\n","<font size=3px>\n","1. Conv(1, 32) -> ReLU -> Dropout -> Maxpool</br>\n","2. Conv(32, 64) -> ReLU -> Dropout -> Maxpool</br>\n","3. Conv(64, 128) -> ReLU -> Dropout -> Maxpool</br>\n","4. Conv(128, 256) -> ReLU -> Dropout -> Maxpool</br>\n","5. Conv(256, 512) -> ReLU -> Dropout -> Maxpool</br>\n","6. Transformer Encoder Layer</br>\n","7. Transformer Encoder Layer</br>\n","8. Transformer Encoder Layer</br>\n","9. Fully Connected Layer (512, 62)</br>\n","</font>\n","\n","<font color='blue'>Optimizer</font>: Adam (lr: 0.0001)</br>\n","<font color='blue'>Loss</font>: CrossEntropyLoss</br>\n","\n","\n","**Conclusions**</br>\n","At epoch 29:</br>\n","Without Augmentation:</br>\n","Training Accuracy: 97.53%\t\n","Validation Accuracy: 72.90%</br>\n","\n","With Augmentation:</br>\n","Training Accuracy: 93.99% \t\n","Validation Accuracy: 77.90%\n","\n","If the notebook is being run on the local system, please download the necessary files from the drive link provided in the code cells.</br>\n","<font color=\"blue\">*Please change the links accordingly*</font>\n","\n","<font color='grey'>*The following code is implemented in PyTorch.*</font>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nzkdOIZnCNO_"},"source":["#Import Libraries"]},{"cell_type":"code","metadata":{"id":"S0c8ZzkiRKX0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_C2YlG3bIjJ"},"source":["!pip install pycm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AF1XGmUlRQp2"},"source":["# Point 1's data is accessed via the zip file stored in the drive\n","# This zip file is transferred to the disk of the google colab, \n","# because accessing it from the disk directly is faster than\n","# accessing the images from the drive\n","\n","# Link to the zip file: https://drive.google.com/file/d/1g4dHphWLCX1PisdXacrfw8kdn7MGoxv5/view?usp=sharing\n","!cp -r \"/content/drive/MyDrive/MIDAS/Point1/train.zip\" \"/content/train.zip\" \n","!unzip train.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0Ovh-DxRSLt"},"source":["import numpy as np\n","import torch\n","import csv\n","from torch import nn\n","import pandas as pd\n","import cv2 as cv\n","from torch.utils.data import DataLoader, Dataset\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, ZeroPad2d\n","from torchvision import transforms\n","import shutil\n","from sklearn.metrics import auc, confusion_matrix, classification_report\n","import seaborn as sns\n","from scipy.optimize import brentq\n","from scipy.interpolate import interp1d\n","import random\n","from pycm import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OcY-JPkWCW63"},"source":["#Data Preparation\n","\n","Following factors have been taken care of in this stage:\n","\n","\n","*   Image is resized to 200 * 200\n","*   Augmentation may or may not be performed on the training data\n","*   All images have been normalised\n","\n","The only augmentation that is performed is Rotation.\n","The explanation for this is that the performance degraded on trying other augmentations like translation and shearing.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"bvdZMDbpRTiF"},"source":["#All images are resized to 200 * 200 and have been normalized\n","def _preprocess(image):\n","    # Preprocessing step\n","    img_transform = transforms.Compose([\n","        transforms.ToPILImage(),              #Conversion to PIL Image\n","        transforms.Resize((200, 200)),        #Resize image to 200 * 200\n","        transforms.ToTensor(),                #Conversion to Tensor\n","        transforms.Normalize((0.5, ), (0.5,)) #Normalise Image\n","    ])\n","    return img_transform(image)\n","\n","#Used for augmenting training data\n","def _preprocess_aug(image):\n","  transform_aug = transforms.Compose([\n","     transforms.ToPILImage(),                 #Conversion to PIL Image                                      \n","     transforms.Resize((200, 200)),           #Resize image to 200 * 200                                                     \n","     transforms.RandomRotation(20),           #Apply rotations upto 20 degrees     \n","     #transforms.RandomAffine(0),\n","     transforms.ToTensor(),                   #Conversion to Tensor\n","     transforms.Normalize((0.5, ), (0.5,))    #Normalise Image\n","    ])\n","\n","  return transform_aug(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QmJv73u-JAmZ"},"source":["Use the below class for augmentation"]},{"cell_type":"code","metadata":{"id":"L54OGSS2flrB"},"source":["#Class used by DataLoader for training data\n","class Images_train(Dataset):\n","  def __init__(self, df):\n","        #DataFrame Structure must be of the form: | FilePath | Label |\n","        self.data = df\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, index):\n","    #print(index)\n","    image1 = cv.imread(self.data.iloc[index, 0], cv.IMREAD_GRAYSCALE) #Column 0: FilePath\n","    temp=random.randint(0,1)            #The temp variable takes care of the fact that not all images are augmented.\n","    if temp == 1:                       #If temp is 1, \n","      image1 = _preprocess_aug(image1)  #augment the image\n","    else:                               #else\n","      image1 = _preprocess(image1)      #No augmentations are to be performed\n","\n","    label = self.data.iloc[index, 1] #Column 1: Label\n","\n","    return image1, torch.from_numpy(np.array([label], dtype=np.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3ToTdFVJJzy"},"source":["Use the below class if augmentation is not required"]},{"cell_type":"code","metadata":{"id":"ep1uASGARZ1X"},"source":["#Class used by DataLoader for training data\n","class Images_train(Dataset):\n","  def __init__(self, df):\n","        #DataFrame Structure must be of the form: | FilePath | Label |\n","        self.data = df\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, index):\n","    #print(index)\n","    image1 = cv.imread(self.data.iloc[index, 0], cv.IMREAD_GRAYSCALE) #Column 0: FilePath\n","    image1 = _preprocess(image1)\n","\n","    label = self.data.iloc[index, 1] #Column 1: Label\n","\n","    return image1, torch.from_numpy(np.array([label], dtype=np.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7AxhWcfRdcB"},"source":["#Class used by DataLoader for testing data\n","class Images_test(Dataset):\n","  def __init__(self, df):\n","        self.data = df\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, index):\n","    #print(index)\n","    image1 = cv.imread(self.data.iloc[index, 0], cv.IMREAD_GRAYSCALE) #Column 0: FilePath\n","    image1 = _preprocess(image1)\n","\n","    label = self.data.iloc[index, 1] #Column 1: Label\n","\n","    return image1, torch.from_numpy(np.array([label], dtype=np.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"NJtbhtB7Redo","executionInfo":{"status":"ok","timestamp":1617790248381,"user_tz":-330,"elapsed":1565,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"3642cd37-72a6-494d-f355-2a43a5206635"},"source":["df = pd.read_csv(\"/content/drive/MyDrive/MIDAS/Point1/pointOne_cpath.csv\")\n","df = df.sample(frac = 1)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FilePath</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>522</th>\n","      <td>/content/train/Sample035/img035-022.png</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>318</th>\n","      <td>/content/train/Sample034/img034-011.png</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>/content/train/Sample040/img040-037.png</td>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>2462</th>\n","      <td>/content/train/Sample012/img012-046.png</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>856</th>\n","      <td>/content/train/Sample018/img018-030.png</td>\n","      <td>17</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                     FilePath  Label\n","522   /content/train/Sample035/img035-022.png     34\n","318   /content/train/Sample034/img034-011.png     33\n","2310  /content/train/Sample040/img040-037.png     39\n","2462  /content/train/Sample012/img012-046.png     11\n","856   /content/train/Sample018/img018-030.png     17"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4V_ModnSRgFg","executionInfo":{"status":"ok","timestamp":1617795135164,"user_tz":-330,"elapsed":1375,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"14bda7f5-9930-4bd7-c86e-7b2c1117c9ac"},"source":["df_train, df_valid = train_test_split(df, test_size=0.25) #Split of 75/25\n","train_dataset = Images_train(df_train)\n","valid_dataset = Images_test(df_valid)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  #Train\n","val_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True)  #Test\n","\n","train_iter = iter(train_loader) \n","images1, labels = train_iter.next()\n","print('images shape on batch size = {}'.format(images1.size()))\n","print('labels shape on batch size = {}'.format(labels.size()))\n","\n","val_iter = iter(val_loader)\n","images1, labels = val_iter.next()\n","print('images shape on batch size = {}'.format(images1.size()))\n","print('labels shape on batch size = {}'.format(labels.size()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["images shape on batch size = torch.Size([8, 1, 200, 200])\n","labels shape on batch size = torch.Size([8, 1])\n","images shape on batch size = torch.Size([1, 1, 200, 200])\n","labels shape on batch size = torch.Size([1, 1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jOcxPG1uLesq"},"source":["#Model Creation\n","\n","The following is the description of the model architecture:\n","<font size=3px>\n","1. Conv(1, 32) -> ReLU -> Dropout -> Maxpool</br>\n","2. Conv(32, 64) -> ReLU -> Dropout -> Maxpool</br>\n","3. Conv(64, 128) -> ReLU -> Dropout -> Maxpool</br>\n","4. Conv(128, 256) -> ReLU -> Dropout -> Maxpool</br>\n","5. Conv(256, 512) -> ReLU -> Dropout -> Maxpool</br>\n","6. Transformer Encoder Layer</br>\n","7. Transformer Encoder Layer</br>\n","8. Transformer Encoder Layer</br>\n","9. Fully Connected Layer (512, 62)</br>\n","</font>\n","\n","##Explanation:\n","The idea is to chain a convolutional neural network (CNN), with a Transformer encoder architecture. This CNN is responsible for extraction of the local information from the image and the Transformer encoder, reasons about the image as a whole and then generates the predictions. \n","\n","The transformer encoder is used to improve the embeddings recieved from the CNN, and the reason why it should succeed is because of the concept of \"Attention\" that is used in them. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_7v1fQqJRi5a"},"source":["class smallModel(nn.Module):\n","  def __init__(self):\n","    super(smallModel, self).__init__()\n","    \n","    self.zp1 = nn.ZeroPad2d(1)\n","    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n","    self.mp1 = nn.MaxPool2d(kernel_size=2)\n","    self.dp = nn.Dropout(p=0.3)\n","\n","    self.zp2 = nn.ZeroPad2d(1)\n","    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","    self.mp2 = nn.MaxPool2d(kernel_size=2)\n","\n","    self.zp3 = nn.ZeroPad2d(1)\n","    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","    self.mp3 = nn.MaxPool2d(kernel_size=2)\n","\n","    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","    self.mp4 = nn.MaxPool2d(kernel_size=2)\n","\n","    self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n","    self.mp5 = nn.MaxPool2d(kernel_size=2)\n","\n","    self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n","    self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3) #num_layers=3: Three encoder layers in one block\n","\n","    self.relu = nn.ReLU()\n","\n","    self.row_emb = nn.Parameter(torch.rand(64, 512 // 2)) #[64, 256]\n","    self.col_emb = nn.Parameter(torch.rand(64, 512 // 2)) #[64, 256]\n","\n","    self.fc4 = nn.Linear(512 * 6 * 6, 62)\n","\n","    #Kindly ignore these layers\n","    self.fc1 = nn.Linear(512 * 6 * 6, 9216)\n","    self.fc2 = nn.Linear(9216, 4096)\n","    self.fc3 = nn.Linear(4096, 1024)\n","\n","\n","  def forward(self, x):\n","    x = self.dp(self.relu(self.conv1(x)))   #Conv -> ReLU -> Dropout: 200 * 200 * 32\n","    x = self.mp1(x)                         #MaxPool: 100 * 100 * 32\n","\n","    x = self.dp(self.relu(self.conv2(x)))   #Conv -> ReLU -> Dropout: 100 * 100 * 64\n","    x = self.mp2(x)                         #MaxPool: 50 * 50 * 64\n","\n","    x = self.dp(self.relu(self.conv3(x)))   #Conv -> ReLU -> Dropout: 50 * 50 * 128\n","    x = self.mp3(x)                         #MaxPool: 25 * 25 * 128\n","\n","    x = self.dp(self.relu(self.conv4(x)))   #Conv -> ReLU -> Dropout: 25 * 25 * 256\n","    x = self.mp4(x)                         #MaxPool: 12 * 12 * 256\n","\n","    x = self.dp(self.relu(self.conv5(x)))   #Conv -> ReLU -> Dropout: 12 * 12 * 512\n","    x = self.mp5(x)                         #MaxPool: 6 * 6 * 512\n","\n","    H = x.shape[-1]#H = 6\n","    W = x.shape[-2]#W = 6\n","\n","   \n","    pos = torch.cat([self.col_emb[:W].unsqueeze(0).repeat(H, 1, 1), self.row_emb[:H].unsqueeze(1).repeat(1, W, 1),], dim=-1).flatten(0, 1).unsqueeze(1)\n","    #Shape of pos: [6 * 6, 1,  512]\n","\n","    x = x.flatten(2).permute(2, 0, 1)\n","\n","    x = self.transformer_encoder(pos + x) \n","\n","    x = x.permute(1, 2, 0)\n","    x = torch.reshape(x, ((x.shape)[0], 512 * 36))\n","\n","    #x = self.relu(self.fc1(x))\n","    #x = self.relu(self.fc2(x))\n","    #x = self.relu(self.fc3(x))\n","    x = self.fc4(x)\n","  \n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_o8-4z38RrXz"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = smallModel().to(device)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKUBtYhfgD_z"},"source":["x = torch.rand(1, 200, 200)\n","x = x.unsqueeze(0)\n","print(x.shape)\n","\n","#model = smallModel()\n","#model.train()\n","pred = model(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nO47iuchnOw"},"source":["#Model Training"]},{"cell_type":"code","metadata":{"id":"zfJKBhxMRtPI"},"source":["#Returns number of elements that are equal in the out (output) and labels (target) tensor\n","def accuracy(out, labels):\n","    count = 0\n","    _,pred = torch.max(out, dim=1)\n","    for i in range(output.shape[0]):\n","      if pred[i] == labels[i][0]:\n","        count = count + 1\n","    return count"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2WtBTAg9i-Xb"},"source":["With Augmentation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIpsjyXHf4GF","executionInfo":{"status":"ok","timestamp":1617793280197,"user_tz":-330,"elapsed":1018096,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"3079e7a8-6d4e-4f25-c1e1-94d356c61d04"},"source":["train_loss_list = []\n","valid_loss_list = []\n","train_accuracy = []\n","valid_accuracy = []\n","\n","epochs = 10\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-4)\n","criterion = nn.CrossEntropyLoss()\n","min_valid_loss = np.Inf\n","check_epoch = 5\n","epoch_no_improve = 0\n","i = 0\n","\n","\n","for epoch in range(1, 30):\n","  train_loss = 0\n","  valid_loss = 0\n","  tcorrect = 0\n","  vcorrect = 0\n","  model.train()\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  print(\"Training: \")\n","  for batch, (img, target) in enumerate(train_loader):\n","    #print(batch)\n","    if torch.cuda.is_available():\n","        img, target = img.to(device=device, dtype=torch.float), target.to(device=device, dtype=torch.long)\n","\n","    optimizer.zero_grad()\n","    output = model(img)\n","\n","    loss = criterion(output, target.view(-1))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    train_loss += loss.item()\n","    tcorrect += accuracy(output, target)\n","    _,pred = torch.max(output, dim=1)\n","    \"\"\"print('Pred: ')\n","    print(pred)\n","    print('Target: ')\n","    print(target)\"\"\"\n","  \n","    \n","  model.eval()\n","  print(\"Validation: \")\n","  for batch, (img, target) in enumerate(val_loader):\n","    if torch.cuda.is_available():\n","        img, target = img.to(device=device, dtype=torch.float), target.to(device=device, dtype=torch.long)\n","\n","    output = model(img)\n","    \n","    loss = criterion(output, target.view(-1))\n","\n","    valid_loss += loss.item()\n","    vcorrect += accuracy(output, target)\n","    _,pred = torch.max(output, dim=1)\n","    \"\"\"print('Pred: ')\n","    print(pred)\n","    print('Target: ')\n","    print(target)\"\"\"\n","    \n","  # calculate average losses\n","  train_loss = train_loss/len(train_loader)\n","  valid_loss = valid_loss/len(val_loader)\n","  train_loss_list.append(train_loss)\n","  valid_loss_list.append(valid_loss)\n","\n","  tcorrect = tcorrect/(len(train_loader) * 8)\n","  vcorrect = vcorrect/len(val_loader)\n","  train_accuracy.append(tcorrect)\n","  valid_accuracy.append(vcorrect)\n","  \n","  # print training/validation statistics \n","  print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n","      epoch, train_loss, tcorrect, valid_loss, vcorrect))\n","  #print(train_loss_list)\n","  #print(valid_loss_list)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training: \n","Validation: \n","Epoch: 1 \tTraining Loss: 4.803071 \tTraining Accuracy: 0.009120 \tValidation Loss: 4.427864 \tValidation Accuracy: 0.016129\n","Training: \n","Validation: \n","Epoch: 2 \tTraining Loss: 4.448962 \tTraining Accuracy: 0.017167 \tValidation Loss: 4.329213 \tValidation Accuracy: 0.022581\n","Training: \n","Validation: \n","Epoch: 3 \tTraining Loss: 4.368277 \tTraining Accuracy: 0.012876 \tValidation Loss: 4.279251 \tValidation Accuracy: 0.022581\n","Training: \n","Validation: \n","Epoch: 4 \tTraining Loss: 4.309457 \tTraining Accuracy: 0.014485 \tValidation Loss: 4.264051 \tValidation Accuracy: 0.019355\n","Training: \n","Validation: \n","Epoch: 5 \tTraining Loss: 3.944825 \tTraining Accuracy: 0.070815 \tValidation Loss: 3.794700 \tValidation Accuracy: 0.082258\n","Training: \n","Validation: \n","Epoch: 6 \tTraining Loss: 3.025293 \tTraining Accuracy: 0.233906 \tValidation Loss: 2.586340 \tValidation Accuracy: 0.311290\n","Training: \n","Validation: \n","Epoch: 7 \tTraining Loss: 2.019408 \tTraining Accuracy: 0.435622 \tValidation Loss: 1.715645 \tValidation Accuracy: 0.491935\n","Training: \n","Validation: \n","Epoch: 8 \tTraining Loss: 1.492178 \tTraining Accuracy: 0.573498 \tValidation Loss: 2.324620 \tValidation Accuracy: 0.343548\n","Training: \n","Validation: \n","Epoch: 9 \tTraining Loss: 1.107613 \tTraining Accuracy: 0.666309 \tValidation Loss: 2.155924 \tValidation Accuracy: 0.401613\n","Training: \n","Validation: \n","Epoch: 10 \tTraining Loss: 0.922411 \tTraining Accuracy: 0.719957 \tValidation Loss: 1.380671 \tValidation Accuracy: 0.611290\n","Training: \n","Validation: \n","Epoch: 11 \tTraining Loss: 0.734726 \tTraining Accuracy: 0.762339 \tValidation Loss: 1.292339 \tValidation Accuracy: 0.633871\n","Training: \n","Validation: \n","Epoch: 12 \tTraining Loss: 0.606451 \tTraining Accuracy: 0.803648 \tValidation Loss: 1.214134 \tValidation Accuracy: 0.645161\n","Training: \n","Validation: \n","Epoch: 13 \tTraining Loss: 0.492621 \tTraining Accuracy: 0.837446 \tValidation Loss: 1.264126 \tValidation Accuracy: 0.616129\n","Training: \n","Validation: \n","Epoch: 14 \tTraining Loss: 0.440455 \tTraining Accuracy: 0.849785 \tValidation Loss: 1.279717 \tValidation Accuracy: 0.653226\n","Training: \n","Validation: \n","Epoch: 15 \tTraining Loss: 0.424274 \tTraining Accuracy: 0.855150 \tValidation Loss: 1.167257 \tValidation Accuracy: 0.670968\n","Training: \n","Validation: \n","Epoch: 16 \tTraining Loss: 0.331119 \tTraining Accuracy: 0.886803 \tValidation Loss: 1.138694 \tValidation Accuracy: 0.677419\n","Training: \n","Validation: \n","Epoch: 17 \tTraining Loss: 0.319768 \tTraining Accuracy: 0.892704 \tValidation Loss: 0.936025 \tValidation Accuracy: 0.750000\n","Training: \n","Validation: \n","Epoch: 18 \tTraining Loss: 0.314176 \tTraining Accuracy: 0.898069 \tValidation Loss: 1.097310 \tValidation Accuracy: 0.706452\n","Training: \n","Validation: \n","Epoch: 19 \tTraining Loss: 0.263633 \tTraining Accuracy: 0.915236 \tValidation Loss: 0.999164 \tValidation Accuracy: 0.720968\n","Training: \n","Validation: \n","Epoch: 20 \tTraining Loss: 0.284069 \tTraining Accuracy: 0.904506 \tValidation Loss: 0.982020 \tValidation Accuracy: 0.722581\n","Training: \n","Validation: \n","Epoch: 21 \tTraining Loss: 0.246717 \tTraining Accuracy: 0.920601 \tValidation Loss: 0.916848 \tValidation Accuracy: 0.753226\n","Training: \n","Validation: \n","Epoch: 22 \tTraining Loss: 0.219073 \tTraining Accuracy: 0.922210 \tValidation Loss: 0.938339 \tValidation Accuracy: 0.717742\n","Training: \n","Validation: \n","Epoch: 23 \tTraining Loss: 0.263187 \tTraining Accuracy: 0.908262 \tValidation Loss: 1.202176 \tValidation Accuracy: 0.691935\n","Training: \n","Validation: \n","Epoch: 24 \tTraining Loss: 0.197488 \tTraining Accuracy: 0.933476 \tValidation Loss: 1.085047 \tValidation Accuracy: 0.738710\n","Training: \n","Validation: \n","Epoch: 25 \tTraining Loss: 0.219210 \tTraining Accuracy: 0.923820 \tValidation Loss: 1.238530 \tValidation Accuracy: 0.709677\n","Training: \n","Validation: \n","Epoch: 26 \tTraining Loss: 0.234537 \tTraining Accuracy: 0.912554 \tValidation Loss: 1.015155 \tValidation Accuracy: 0.738710\n","Training: \n","Validation: \n","Epoch: 27 \tTraining Loss: 0.185451 \tTraining Accuracy: 0.942597 \tValidation Loss: 1.094548 \tValidation Accuracy: 0.711290\n","Training: \n","Validation: \n","Epoch: 28 \tTraining Loss: 0.207864 \tTraining Accuracy: 0.930794 \tValidation Loss: 0.979444 \tValidation Accuracy: 0.748387\n","Training: \n","Validation: \n","Epoch: 29 \tTraining Loss: 0.168917 \tTraining Accuracy: 0.939914 \tValidation Loss: 0.918426 \tValidation Accuracy: 0.779032\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xmlj2PX3YIOv"},"source":["chkpt_path = \"/content/drive/MyDrive/MIDAS/Point1/model_encoder_aug.pt\" \n","torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, chkpt_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZNK4U0JjDEv"},"source":["Without Augmentation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sf2dUgCw_vfq","executionInfo":{"status":"ok","timestamp":1617794537490,"user_tz":-330,"elapsed":1023728,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"6a079bbf-10df-480d-a819-8c6c4f24d04f"},"source":["train_loss_list = []\n","valid_loss_list = []\n","train_accuracy = []\n","valid_accuracy = []\n","\n","epochs = 10\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-4)\n","criterion = nn.CrossEntropyLoss()\n","min_valid_loss = np.Inf\n","check_epoch = 5\n","epoch_no_improve = 0\n","i = 0\n","\n","\n","for epoch in range(1, 30):\n","  train_loss = 0\n","  valid_loss = 0\n","  tcorrect = 0\n","  vcorrect = 0\n","  model.train()\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  print(\"Training: \")\n","  for batch, (img, target) in enumerate(train_loader):\n","    #print(batch)\n","    if torch.cuda.is_available():\n","        img, target = img.to(device=device, dtype=torch.float), target.to(device=device, dtype=torch.long)\n","\n","    optimizer.zero_grad()\n","    output = model(img)\n","\n","    loss = criterion(output, target.view(-1))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    train_loss += loss.item()\n","    tcorrect += accuracy(output, target)\n","    _,pred = torch.max(output, dim=1)\n","    \"\"\"print('Pred: ')\n","    print(pred)\n","    print('Target: ')\n","    print(target)\"\"\"\n","  \n","    \n","  model.eval()\n","  print(\"Validation: \")\n","  for batch, (img, target) in enumerate(val_loader):\n","    if torch.cuda.is_available():\n","        img, target = img.to(device=device, dtype=torch.float), target.to(device=device, dtype=torch.long)\n","\n","    output = model(img)\n","    \n","    loss = criterion(output, target.view(-1))\n","\n","    valid_loss += loss.item()\n","    vcorrect += accuracy(output, target)\n","    _,pred = torch.max(output, dim=1)\n","    \"\"\"print('Pred: ')\n","    print(pred)\n","    print('Target: ')\n","    print(target)\"\"\"\n","    \n","  # calculate average losses\n","  train_loss = train_loss/len(train_loader)\n","  valid_loss = valid_loss/len(val_loader)\n","  train_loss_list.append(train_loss)\n","  valid_loss_list.append(valid_loss)\n","\n","  tcorrect = tcorrect/(len(train_loader) * 8)\n","  vcorrect = vcorrect/len(val_loader)\n","  train_accuracy.append(tcorrect)\n","  valid_accuracy.append(vcorrect)\n","  \n","  # print training/validation statistics \n","  print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n","      epoch, train_loss, tcorrect, valid_loss, vcorrect))\n","  #print(train_loss_list)\n","  #print(valid_loss_list)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training: \n","Validation: \n","Epoch: 1 \tTraining Loss: 4.796488 \tTraining Accuracy: 0.017704 \tValidation Loss: 4.579155 \tValidation Accuracy: 0.017742\n","Training: \n","Validation: \n","Epoch: 2 \tTraining Loss: 4.445758 \tTraining Accuracy: 0.017704 \tValidation Loss: 4.485931 \tValidation Accuracy: 0.017742\n","Training: \n","Validation: \n","Epoch: 3 \tTraining Loss: 4.128207 \tTraining Accuracy: 0.049356 \tValidation Loss: 3.649084 \tValidation Accuracy: 0.132258\n","Training: \n","Validation: \n","Epoch: 4 \tTraining Loss: 2.574164 \tTraining Accuracy: 0.340129 \tValidation Loss: 2.369130 \tValidation Accuracy: 0.369355\n","Training: \n","Validation: \n","Epoch: 5 \tTraining Loss: 1.354532 \tTraining Accuracy: 0.609979 \tValidation Loss: 1.815850 \tValidation Accuracy: 0.475806\n","Training: \n","Validation: \n","Epoch: 6 \tTraining Loss: 0.895825 \tTraining Accuracy: 0.731760 \tValidation Loss: 1.299558 \tValidation Accuracy: 0.627419\n","Training: \n","Validation: \n","Epoch: 7 \tTraining Loss: 0.593726 \tTraining Accuracy: 0.816524 \tValidation Loss: 1.186498 \tValidation Accuracy: 0.651613\n","Training: \n","Validation: \n","Epoch: 8 \tTraining Loss: 0.384243 \tTraining Accuracy: 0.883047 \tValidation Loss: 1.053205 \tValidation Accuracy: 0.704839\n","Training: \n","Validation: \n","Epoch: 9 \tTraining Loss: 0.280713 \tTraining Accuracy: 0.910408 \tValidation Loss: 1.109442 \tValidation Accuracy: 0.680645\n","Training: \n","Validation: \n","Epoch: 10 \tTraining Loss: 0.216595 \tTraining Accuracy: 0.929185 \tValidation Loss: 1.513338 \tValidation Accuracy: 0.579032\n","Training: \n","Validation: \n","Epoch: 11 \tTraining Loss: 0.205112 \tTraining Accuracy: 0.938841 \tValidation Loss: 1.347229 \tValidation Accuracy: 0.630645\n","Training: \n","Validation: \n","Epoch: 12 \tTraining Loss: 0.135724 \tTraining Accuracy: 0.955472 \tValidation Loss: 1.030194 \tValidation Accuracy: 0.714516\n","Training: \n","Validation: \n","Epoch: 13 \tTraining Loss: 0.123909 \tTraining Accuracy: 0.959764 \tValidation Loss: 1.125646 \tValidation Accuracy: 0.690323\n","Training: \n","Validation: \n","Epoch: 14 \tTraining Loss: 0.124406 \tTraining Accuracy: 0.961910 \tValidation Loss: 1.070146 \tValidation Accuracy: 0.727419\n","Training: \n","Validation: \n","Epoch: 15 \tTraining Loss: 0.109623 \tTraining Accuracy: 0.962446 \tValidation Loss: 0.990050 \tValidation Accuracy: 0.730645\n","Training: \n","Validation: \n","Epoch: 16 \tTraining Loss: 0.094339 \tTraining Accuracy: 0.971567 \tValidation Loss: 1.131980 \tValidation Accuracy: 0.704839\n","Training: \n","Validation: \n","Epoch: 17 \tTraining Loss: 0.094665 \tTraining Accuracy: 0.968348 \tValidation Loss: 1.078759 \tValidation Accuracy: 0.727419\n","Training: \n","Validation: \n","Epoch: 18 \tTraining Loss: 0.117271 \tTraining Accuracy: 0.958691 \tValidation Loss: 1.213544 \tValidation Accuracy: 0.685484\n","Training: \n","Validation: \n","Epoch: 19 \tTraining Loss: 0.102378 \tTraining Accuracy: 0.964592 \tValidation Loss: 1.191076 \tValidation Accuracy: 0.704839\n","Training: \n","Validation: \n","Epoch: 20 \tTraining Loss: 0.077723 \tTraining Accuracy: 0.975322 \tValidation Loss: 1.121682 \tValidation Accuracy: 0.729032\n","Training: \n","Validation: \n","Epoch: 21 \tTraining Loss: 0.083598 \tTraining Accuracy: 0.975322 \tValidation Loss: 1.155654 \tValidation Accuracy: 0.708065\n","Training: \n","Validation: \n","Epoch: 22 \tTraining Loss: 0.057341 \tTraining Accuracy: 0.981760 \tValidation Loss: 1.229709 \tValidation Accuracy: 0.727419\n","Training: \n","Validation: \n","Epoch: 23 \tTraining Loss: 0.066815 \tTraining Accuracy: 0.973712 \tValidation Loss: 1.418565 \tValidation Accuracy: 0.640323\n","Training: \n","Validation: \n","Epoch: 24 \tTraining Loss: 0.114453 \tTraining Accuracy: 0.961373 \tValidation Loss: 1.202342 \tValidation Accuracy: 0.720968\n","Training: \n","Validation: \n","Epoch: 25 \tTraining Loss: 0.068412 \tTraining Accuracy: 0.976931 \tValidation Loss: 1.229959 \tValidation Accuracy: 0.727419\n","Training: \n","Validation: \n","Epoch: 26 \tTraining Loss: 0.045624 \tTraining Accuracy: 0.984979 \tValidation Loss: 1.245239 \tValidation Accuracy: 0.709677\n","Training: \n","Validation: \n","Epoch: 27 \tTraining Loss: 0.084141 \tTraining Accuracy: 0.973712 \tValidation Loss: 1.457001 \tValidation Accuracy: 0.654839\n","Training: \n","Validation: \n","Epoch: 28 \tTraining Loss: 0.112748 \tTraining Accuracy: 0.964592 \tValidation Loss: 1.484033 \tValidation Accuracy: 0.666129\n","Training: \n","Validation: \n","Epoch: 29 \tTraining Loss: 0.091380 \tTraining Accuracy: 0.968884 \tValidation Loss: 1.333468 \tValidation Accuracy: 0.716129\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rEQpJsMVAfiO"},"source":["chkpt_path = \"model_encoder_final.pt\"\n","torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, chkpt_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xk5jcEqZiGVp"},"source":["#Visualisation and Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"B9vMAFmbSnQ3","executionInfo":{"status":"ok","timestamp":1617782816113,"user_tz":-330,"elapsed":1451,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"b7a0734a-0738-448d-ee10-c52df032bffb"},"source":["#With Augmentation\n","a = [i for  i in range(1, 21)]\n","plt.plot(a, train_loss_list, label = 'Training Loss')\n","plt.plot(a, valid_loss_list, label = 'Validation Loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f4c9ed306d0>"]},"metadata":{"tags":[]},"execution_count":18},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"l3qSwpgft0G4","executionInfo":{"status":"ok","timestamp":1617793303970,"user_tz":-330,"elapsed":1275,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"5dfc4ad2-d6b4-4f50-e9d9-f1b6aed3e4e6"},"source":["#Without Augmentation\n","a = [i for  i in range(1, 30)]\n","plt.plot(a, train_loss_list, label = 'Training Loss')\n","plt.plot(a, valid_loss_list, label = 'Validation Loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f4bbba58490>"]},"metadata":{"tags":[]},"execution_count":110},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"C6IUYVoRSHFQ","executionInfo":{"status":"ok","timestamp":1617799266310,"user_tz":-330,"elapsed":10591,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"d22fddad-9200-4fcf-9b37-954c6bb50a31"},"source":["#Evaluation begins\n","i = 0\n","l1 = []\n","l2 = []\n","\n","model.eval()\n","test_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n","for batch, (img, target) in enumerate(test_loader):\n","    if torch.cuda.is_available():\n","        img, target = img.to(device=device, dtype=torch.float), target.to(device=device, dtype=torch.long)\n","\n","    output = model(img)\n","    \n","    vcorrect += accuracy(output, target)\n","    _,pred = torch.max(output, dim=1)\n","    l1.append(pred.item())\n","    l2.append(target.item)\n","\n","df = pd.DataFrame(columns=[\"Pred\", \"Target\"])\n","df[\"Pred\"] = l1\n","df[\"Target\"] = l2\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pred</th>\n","      <th>Target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>22</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>40</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>47</td>\n","      <td>18</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pred  Target\n","0    60      60\n","1    22      22\n","2    40      40\n","3     3       9\n","4    47      18"]},"metadata":{"tags":[]},"execution_count":151}]},{"cell_type":"code","metadata":{"id":"kTEmSNkHTXub"},"source":["y = np.array(df[\"Target\"])\n","y_pred = np.array(df[\"Pred\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HgmSF8JTrvx","executionInfo":{"status":"ok","timestamp":1617798821296,"user_tz":-330,"elapsed":1449,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"9ac6293c-06ce-4ebf-b44c-9616432184f2"},"source":["print(\"Classification Report:\")\n","print(classification_report(y, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.25      0.12      0.17         8\n","           1       0.38      0.50      0.43         6\n","           2       0.86      1.00      0.92         6\n","           3       0.92      0.86      0.89        14\n","           4       0.90      0.69      0.78        13\n","           5       1.00      0.75      0.86         8\n","           6       0.80      0.73      0.76        11\n","           7       1.00      0.64      0.78        11\n","           8       0.70      0.88      0.78         8\n","           9       0.83      0.56      0.67         9\n","          10       0.70      0.88      0.78         8\n","          11       0.50      0.67      0.57         6\n","          12       0.93      0.81      0.87        16\n","          13       0.82      0.75      0.78        12\n","          14       1.00      0.78      0.88         9\n","          15       1.00      1.00      1.00        10\n","          16       1.00      0.73      0.84        11\n","          17       0.90      0.90      0.90        10\n","          18       0.83      0.50      0.62        10\n","          19       0.77      0.83      0.80        12\n","          20       1.00      0.90      0.95        10\n","          21       0.89      0.89      0.89         9\n","          22       0.92      1.00      0.96        11\n","          23       1.00      0.80      0.89        10\n","          24       0.69      0.90      0.78        10\n","          25       0.90      0.82      0.86        11\n","          26       1.00      0.67      0.80        15\n","          27       0.73      1.00      0.84         8\n","          28       0.75      0.75      0.75         8\n","          29       0.57      0.89      0.70         9\n","          30       0.70      0.70      0.70        10\n","          31       0.70      0.78      0.74         9\n","          32       0.89      0.73      0.80        11\n","          33       1.00      0.82      0.90        11\n","          34       0.78      1.00      0.88         7\n","          35       0.71      0.62      0.67         8\n","          36       0.60      0.90      0.72        10\n","          37       0.47      1.00      0.64         8\n","          38       0.44      0.73      0.55        11\n","          39       0.93      0.93      0.93        14\n","          40       0.50      0.50      0.50        10\n","          41       0.75      0.60      0.67        10\n","          42       0.45      0.71      0.56         7\n","          43       0.73      1.00      0.84         8\n","          44       1.00      0.56      0.71         9\n","          45       0.83      0.91      0.87        11\n","          46       0.78      0.78      0.78         9\n","          47       0.67      0.86      0.75         7\n","          48       1.00      0.20      0.33        10\n","          49       0.53      0.94      0.68        17\n","          50       0.53      0.80      0.64        10\n","          51       0.64      0.90      0.75        10\n","          52       1.00      0.46      0.63        13\n","          53       0.83      0.56      0.67         9\n","          54       0.40      0.33      0.36         6\n","          55       0.88      0.88      0.88         8\n","          56       0.75      0.94      0.83        16\n","          57       1.00      0.64      0.78        11\n","          58       0.88      0.58      0.70        12\n","          59       0.71      0.83      0.77         6\n","          60       1.00      0.89      0.94         9\n","          61       0.78      0.50      0.61        14\n","\n","    accuracy                           0.76       620\n","   macro avg       0.78      0.75      0.75       620\n","weighted avg       0.80      0.76      0.75       620\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhTTaOZTT1b9","executionInfo":{"status":"ok","timestamp":1617799376447,"user_tz":-330,"elapsed":2690,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"76570368-b989-48e8-d6e2-f0ed0f1a21f9"},"source":["print(list(y))\n","print(list(y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[50, 44, 49, 8, 36, 34, 16, 52, 9, 29, 0, 32, 56, 8, 29, 32, 45, 18, 39, 51, 47, 59, 19, 20, 52, 19, 36, 57, 1, 49, 3, 53, 12, 54, 54, 16, 14, 56, 28, 23, 39, 43, 22, 42, 19, 48, 61, 58, 2, 23, 52, 50, 48, 28, 22, 34, 56, 24, 26, 45, 19, 47, 17, 17, 12, 0, 39, 40, 16, 12, 25, 20, 55, 0, 8, 50, 38, 21, 49, 37, 44, 6, 55, 35, 3, 29, 35, 59, 26, 17, 55, 38, 55, 39, 17, 19, 40, 13, 1, 3, 21, 39, 25, 52, 12, 12, 36, 33, 4, 57, 15, 39, 53, 45, 14, 23, 13, 20, 14, 16, 61, 2, 50, 27, 45, 49, 3, 51, 35, 24, 32, 4, 59, 41, 54, 20, 18, 0, 35, 4, 4, 1, 12, 43, 23, 20, 40, 51, 43, 22, 56, 7, 13, 56, 25, 49, 6, 57, 28, 34, 36, 38, 48, 2, 55, 51, 61, 31, 56, 32, 12, 2, 30, 23, 21, 26, 29, 38, 29, 34, 4, 31, 4, 23, 31, 6, 9, 47, 42, 51, 40, 12, 50, 42, 15, 44, 41, 7, 15, 30, 51, 15, 27, 56, 6, 57, 49, 9, 38, 53, 39, 58, 19, 10, 50, 41, 7, 27, 60, 23, 45, 11, 25, 16, 49, 60, 7, 22, 39, 3, 53, 12, 5, 13, 13, 35, 31, 15, 28, 12, 37, 23, 58, 32, 49, 52, 30, 9, 21, 22, 32, 30, 27, 55, 60, 26, 14, 49, 4, 39, 37, 26, 8, 60, 56, 49, 61, 10, 16, 21, 18, 26, 47, 56, 12, 40, 56, 58, 12, 28, 6, 38, 26, 61, 16, 48, 48, 28, 24, 9, 28, 21, 48, 58, 45, 10, 44, 36, 21, 36, 46, 24, 57, 51, 57, 36, 3, 41, 35, 5, 25, 50, 48, 15, 44, 25, 60, 41, 40, 33, 29, 5, 52, 24, 37, 58, 32, 46, 32, 2, 8, 49, 46, 0, 18, 53, 11, 61, 22, 4, 20, 16, 6, 22, 5, 61, 51, 45, 7, 28, 61, 20, 3, 33, 12, 42, 18, 39, 22, 26, 4, 0, 46, 27, 17, 7, 30, 34, 6, 5, 45, 50, 6, 39, 9, 23, 49, 17, 31, 22, 32, 40, 46, 31, 57, 58, 38, 56, 11, 49, 13, 33, 9, 58, 18, 60, 4, 60, 35, 23, 60, 49, 59, 38, 26, 11, 57, 54, 19, 22, 52, 26, 34, 39, 61, 46, 43, 20, 3, 3, 49, 37, 14, 59, 33, 31, 7, 16, 34, 38, 26, 18, 53, 43, 26, 61, 8, 21, 39, 15, 41, 26, 6, 56, 46, 25, 18, 38, 12, 10, 52, 16, 6, 61, 0, 1, 33, 45, 9, 11, 20, 15, 53, 29, 54, 53, 14, 20, 49, 19, 24, 37, 30, 15, 4, 31, 50, 0, 40, 17, 48, 61, 25, 47, 52, 13, 52, 31, 57, 22, 15, 33, 40, 36, 59, 61, 24, 3, 6, 53, 19, 12, 10, 26, 47, 1, 29, 7, 55, 13, 30, 30, 10, 24, 43, 27, 43, 1, 17, 41, 44, 57, 32, 61, 19, 12, 36, 18, 47, 60, 4, 33, 19, 32, 26, 57, 46, 52, 56, 41, 5, 50, 17, 4, 35, 29, 42, 9, 14, 17, 38, 13, 37, 24, 13, 5, 58, 52, 25, 3, 24, 7, 3, 11, 14, 55, 33, 16, 56, 45, 48, 48, 27, 10, 8, 8, 7, 13, 30, 30, 39, 58, 37, 10, 44, 44, 43, 33, 56, 40, 49, 41, 54, 25, 19, 42, 51, 58, 3, 45, 52, 25, 27, 7, 36, 33, 51, 2, 14, 21, 44, 58, 42, 46, 3, 18, 56, 5, 13, 41]\n","[0, 1, 49, 8, 36, 34, 16, 52, 9, 29, 24, 56, 56, 8, 29, 32, 45, 18, 39, 51, 47, 59, 19, 20, 52, 19, 36, 57, 1, 43, 3, 53, 12, 54, 6, 16, 14, 56, 28, 49, 39, 43, 22, 37, 19, 48, 61, 19, 2, 23, 52, 50, 49, 28, 22, 34, 56, 24, 36, 45, 19, 47, 17, 17, 38, 50, 39, 38, 16, 12, 13, 20, 38, 0, 8, 40, 38, 21, 49, 37, 44, 6, 55, 35, 3, 29, 61, 59, 26, 17, 55, 38, 55, 39, 17, 19, 40, 13, 39, 3, 21, 39, 51, 42, 12, 12, 36, 33, 4, 57, 15, 39, 53, 19, 38, 23, 13, 20, 14, 37, 18, 2, 50, 27, 45, 49, 11, 51, 40, 24, 30, 49, 59, 41, 42, 20, 18, 24, 35, 10, 10, 1, 12, 43, 30, 20, 40, 51, 43, 22, 56, 29, 13, 56, 25, 49, 6, 31, 28, 34, 38, 38, 49, 2, 55, 51, 61, 31, 56, 58, 12, 2, 30, 23, 21, 49, 29, 54, 29, 34, 4, 31, 4, 23, 31, 6, 51, 47, 42, 51, 38, 12, 50, 42, 15, 44, 41, 29, 15, 30, 51, 15, 27, 56, 6, 31, 49, 9, 38, 49, 39, 56, 19, 41, 50, 41, 7, 27, 60, 23, 45, 11, 25, 16, 49, 60, 7, 22, 39, 3, 49, 12, 5, 13, 11, 35, 34, 15, 54, 38, 37, 23, 58, 32, 49, 51, 37, 9, 21, 22, 32, 30, 27, 55, 60, 26, 14, 49, 4, 39, 37, 26, 8, 60, 56, 49, 61, 10, 16, 21, 47, 26, 46, 56, 12, 36, 36, 58, 38, 28, 6, 38, 26, 40, 16, 49, 49, 37, 0, 9, 28, 21, 49, 58, 45, 10, 6, 36, 38, 36, 43, 24, 57, 51, 57, 36, 3, 11, 35, 5, 25, 50, 48, 15, 44, 25, 60, 55, 50, 33, 29, 5, 36, 24, 37, 32, 32, 46, 32, 2, 11, 49, 46, 24, 18, 53, 11, 61, 22, 4, 20, 50, 6, 22, 28, 42, 51, 45, 25, 28, 35, 20, 3, 33, 12, 0, 1, 39, 22, 26, 4, 24, 27, 27, 17, 7, 30, 34, 6, 5, 45, 50, 37, 42, 1, 23, 49, 17, 31, 22, 32, 40, 46, 30, 31, 58, 38, 56, 8, 49, 13, 33, 4, 58, 47, 60, 4, 60, 61, 23, 60, 49, 59, 38, 26, 11, 57, 54, 19, 22, 56, 26, 34, 39, 3, 46, 43, 46, 3, 3, 49, 37, 14, 59, 59, 31, 7, 16, 34, 12, 26, 18, 53, 43, 27, 61, 8, 21, 39, 15, 53, 36, 37, 56, 46, 25, 1, 38, 12, 10, 52, 16, 6, 61, 50, 47, 49, 45, 9, 13, 20, 15, 59, 29, 42, 53, 14, 20, 49, 19, 24, 37, 30, 15, 49, 31, 50, 50, 38, 17, 49, 61, 25, 47, 52, 13, 36, 31, 37, 22, 15, 33, 40, 36, 54, 8, 24, 3, 37, 29, 19, 12, 10, 26, 47, 41, 1, 7, 55, 50, 30, 56, 10, 24, 43, 27, 43, 1, 17, 10, 44, 57, 32, 35, 19, 12, 36, 18, 47, 34, 4, 33, 45, 32, 27, 57, 46, 51, 56, 41, 5, 50, 17, 4, 35, 29, 42, 42, 14, 22, 40, 13, 37, 24, 13, 28, 56, 51, 25, 50, 24, 7, 3, 11, 40, 55, 33, 9, 56, 45, 49, 43, 27, 10, 8, 8, 7, 13, 21, 30, 39, 58, 37, 10, 29, 44, 43, 33, 56, 40, 49, 41, 8, 25, 45, 42, 37, 17, 3, 45, 52, 25, 27, 29, 36, 33, 51, 2, 14, 21, 2, 58, 42, 46, 3, 29, 56, 5, 19, 41]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lKsvlryUdzP","executionInfo":{"status":"ok","timestamp":1617800257349,"user_tz":-330,"elapsed":3152,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"253a72e6-3c85-4e95-dddc-b55ce65e2f3f"},"source":["(y == y_pred).sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["469"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ubt6QV4FXazi","executionInfo":{"status":"ok","timestamp":1617800722741,"user_tz":-330,"elapsed":1786,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"757c477a-4e73-46b5-fdff-00029e7fb51b"},"source":["cm = ConfusionMatrix(actual_vector=y, predict_vector=y_pred)\n","print(cm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predict  0        1        2        3        4        5        6        7        8        9        10       11       12       13       14       15       16       17       18       19       20       21       22       23       24       25       26       27       28       29       30       31       32       33       34       35       36       37       38       39       40       41       42       43       44       45       46       47       48       49       50       51       52       53       54       55       56       57       58       59       60       61       \n","Actual\n","0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        4        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        0        0        0        0        0        0        0        0        \n","\n","1        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        1        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","2        0        0        6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","3        0        0        0        12       0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        \n","\n","4        0        0        0        0        9        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","5        0        0        0        0        0        6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","6        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","7        0        0        0        0        0        0        0        7        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","8        0        0        0        0        0        0        0        0        7        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","9        0        1        0        0        1        0        0        0        0        5        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        \n","\n","10       0        0        0        0        0        0        0        0        0        0        7        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","11       0        0        0        0        0        0        0        0        1        0        0        4        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","12       0        0        0        0        0        0        0        0        0        0        0        0        13       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","13       0        0        0        0        0        0        0        0        0        0        0        1        0        9        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        \n","\n","14       0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","15       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        10       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","16       0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        \n","\n","17       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","18       0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        5        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","19       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        10       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","20       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","21       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","22       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        11       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","23       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","24       1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","25       0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        \n","\n","26       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        10       2        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","27       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","28       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        6        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        \n","\n","29       0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","30       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        7        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        \n","\n","31       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        7        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","32       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        1        0        0        0        \n","\n","33       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        1        0        0        \n","\n","34       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","35       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        5        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        \n","\n","36       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","37       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","38       0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        \n","\n","39       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        13       0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","40       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        3        0        5        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        \n","\n","41       0        0        0        0        0        0        0        0        0        0        1        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        6        0        0        0        0        0        0        0        0        0        0        0        1        0        1        0        0        0        0        0        0        \n","\n","42       1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        5        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","43       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","44       0        1        1        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        5        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","45       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        10       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","46       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        7        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","47       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","48       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        2        7        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","49       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        16       0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","50       1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        \n","\n","51       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        \n","\n","52       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        1        0        0        0        0        0        0        0        0        3        6        0        0        0        1        0        0        0        0        0        \n","\n","53       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        5        0        0        0        0        0        1        0        0        \n","\n","54       0        0        0        0        0        0        1        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        \n","\n","55       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        0        0        0        0        0        0        \n","\n","56       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        15       0        0        0        0        0        \n","\n","57       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        0        0        0        0        \n","\n","58       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        1        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        7        0        0        0        \n","\n","59       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        5        0        0        \n","\n","60       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        \n","\n","61       0        0        0        1        0        0        0        0        1        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        1        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        \n","\n","\n","\n","\n","\n","Overall Statistics : \n","\n","95% CI                                                            (0.72267,0.79024)\n","ACC Macro                                                         0.99214\n","ARI                                                               0.57415\n","AUNP                                                              0.8762\n","AUNU                                                              0.87492\n","Bangdiwala B                                                      0.61481\n","Bennett S                                                         0.75246\n","CBA                                                               0.6703\n","CSI                                                               0.53454\n","Chi-Squared                                                       23577.10533\n","Chi-Squared DF                                                    3721\n","Conditional Entropy                                               0.93863\n","Cramer V                                                          0.78956\n","Cross Entropy                                                     6.00364\n","F1 Macro                                                          0.74592\n","F1 Micro                                                          0.75645\n","FNR Macro                                                         0.24616\n","FNR Micro                                                         0.24355\n","FPR Macro                                                         0.00399\n","FPR Micro                                                         0.00399\n","Gwet AC1                                                          0.75246\n","Hamming Loss                                                      0.24355\n","Joint Entropy                                                     6.84875\n","KL Divergence                                                     0.09352\n","Kappa                                                             0.75224\n","Kappa 95% CI                                                      (0.71787,0.78661)\n","Kappa No Prevalence                                               0.5129\n","Kappa Standard Error                                              0.01754\n","Kappa Unbiased                                                    0.75211\n","Krippendorff Alpha                                                0.75231\n","Lambda A                                                          0.74959\n","Lambda B                                                          0.75763\n","Mutual Information                                                4.90281\n","NIR                                                               0.02742\n","Overall ACC                                                       0.75645\n","Overall CEN                                                       0.13155\n","Overall J                                                         (38.28717,0.61754)\n","Overall MCC                                                       0.75305\n","Overall MCEN                                                      0.17752\n","Overall RACC                                                      0.01701\n","Overall RACCU                                                     0.01753\n","P-Value                                                           0.0\n","PPV Macro                                                         0.7807\n","PPV Micro                                                         0.75645\n","Pearson C                                                         0.98711\n","Phi-Squared                                                       38.02759\n","RCI                                                               0.82956\n","RR                                                                10.0\n","Reference Entropy                                                 5.91012\n","Response Entropy                                                  5.84144\n","SOA1(Landis & Koch)                                               Substantial\n","SOA2(Fleiss)                                                      Excellent\n","SOA3(Altman)                                                      Good\n","SOA4(Cicchetti)                                                   Excellent\n","SOA5(Cramer)                                                      Strong\n","SOA6(Matthews)                                                    Strong\n","Scott PI                                                          0.75211\n","Standard Error                                                    0.01724\n","TNR Macro                                                         0.99601\n","TNR Micro                                                         0.99601\n","TPR Macro                                                         0.75384\n","TPR Micro                                                         0.75645\n","Zero-one Loss                                                     151\n","\n","Class Statistics :\n","\n","Classes                                                           0             1             2             3             4             5             6             7             8             9             10            11            12            13            14            15            16            17            18            19            20            21            22            23            24            25            26            27            28            29            30            31            32            33            34            35            36            37            38            39            40            41            42            43            44            45            46            47            48            49            50            51            52            53            54            55            56            57            58            59            60            61            \n","ACC(Accuracy)                                                     0.98387       0.9871        0.99839       0.99516       0.99194       0.99677       0.99194       0.99355       0.99355       0.99194       0.99355       0.99032       0.99355       0.99194       0.99677       1.0           0.99516       0.99677       0.99032       0.99194       0.99839       0.99677       0.99839       0.99677       0.99194       0.99516       0.99194       0.99516       0.99355       0.98871       0.99032       0.99194       0.99355       0.99677       0.99677       0.99194       0.98871       0.98548       0.97903       0.99677       0.98387       0.99032       0.9871        0.99516       0.99355       0.99516       0.99355       0.99355       0.9871        0.97581       0.98548       0.99032       0.98871       0.99194       0.98871       0.99677       0.99032       0.99355       0.99032       0.99516       0.99839       0.98548       \n","AGF(Adjusted F-score)                                             0.3708        0.68275       0.98358       0.93112       0.84956       0.88736       0.85868       0.82625       0.91182       0.76938       0.91182       0.78902       0.91091       0.87132       0.90101       1.0           0.87533       0.94791       0.73468       0.90372       0.95769       0.94204       0.99087       0.91168       0.92023       0.91152       0.84237       0.96401       0.86461       0.89295       0.8346        0.8707        0.86689       0.92023       0.97228       0.79881       0.90304       0.90216       0.80029       0.96283       0.7042        0.78824       0.7988        0.96401       0.77883       0.94398       0.88047       0.89942       0.48541       0.90076       0.85069       0.91152       0.7159        0.76938       0.5854        0.93465       0.94349       0.82625       0.78785       0.89715       0.95284       0.73018       \n","AGM(Adjusted geometric mean)                                      0.67181       0.84734       0.99878       0.96128       0.91397       0.93258       0.92341       0.89796       0.96391       0.87063       0.96391       0.90322       0.94885       0.93001       0.94053       1.0           0.92574       0.97293       0.85126       0.95243       0.97413       0.96999       0.99877       0.94678       0.96931       0.95065       0.90712       0.99633       0.93024       0.96398       0.91419       0.937         0.92458       0.95184       0.99756       0.89233       0.9669        0.98898       0.91406       0.98039       0.84683       0.88412       0.91519       0.99633       0.87175       0.97411       0.93817       0.95913       0.72136       0.96767       0.9385        0.9681        0.83798       0.87063       0.78451       0.9663        0.9778        0.89796       0.8796        0.95386       0.9712        0.84966       \n","AM(Difference between automatic and manual classification)        -4            2             1             -1            -3            -2            -1            -4            2             -3            2             2             -2            -1            -2            0             -3            0             -4            1             -1            0             1             -2            3             -1            -5            3             0             5             0             1             -2            -2            2             -1            5             9             7             0             0             -2            4             3             -4            1             0             2             -8            13            5             4             -7            -3            -1            0             4             -4            -4            1             -1            -5            \n","AUC(Area under the ROC curve)                                     0.56005       0.74593       0.99919       0.92775       0.84533       0.875         0.86199       0.81818       0.93505       0.77696       0.93505       0.83008       0.90542       0.87336       0.88889       1.0           0.86364       0.94918       0.74918       0.9142        0.95          0.94363       0.99918       0.9           0.94672       0.90827       0.83333       0.99755       0.87337       0.93953       0.84754       0.88643       0.86282       0.90909       0.99837       0.81087       0.94508       0.99265       0.85543       0.96346       0.7459        0.79836       0.85225       0.99755       0.77778       0.9529        0.88725       0.92612       0.6           0.95898       0.89426       0.9459        0.73077       0.77696       0.66422       0.93668       0.96461       0.81818       0.79084       0.91504       0.94444       0.74835       \n","AUCI(AUC value interpretation)                                    Poor          Good          Excellent     Excellent     Very Good     Very Good     Very Good     Very Good     Excellent     Good          Excellent     Very Good     Excellent     Very Good     Very Good     Excellent     Very Good     Excellent     Good          Excellent     Excellent     Excellent     Excellent     Excellent     Excellent     Excellent     Very Good     Excellent     Very Good     Excellent     Very Good     Very Good     Very Good     Excellent     Excellent     Very Good     Excellent     Excellent     Very Good     Excellent     Good          Good          Very Good     Excellent     Good          Excellent     Very Good     Excellent     Fair          Excellent     Very Good     Excellent     Good          Good          Fair          Excellent     Excellent     Very Good     Good          Excellent     Excellent     Good          \n","AUPR(Area under the PR curve)                                     0.1875        0.4375        0.92857       0.89011       0.79615       0.875         0.76364       0.81818       0.7875        0.69444       0.7875        0.58333       0.87054       0.78409       0.88889       1.0           0.86364       0.9           0.66667       0.80128       0.95          0.88889       0.95833       0.9           0.79615       0.85909       0.83333       0.86364       0.75          0.73016       0.7           0.73889       0.80808       0.90909       0.88889       0.66964       0.75          0.73529       0.58586       0.92857       0.5           0.675         0.58442       0.86364       0.77778       0.87121       0.77778       0.7619        0.6           0.73725       0.66667       0.77143       0.73077       0.69444       0.36667       0.875         0.84375       0.81818       0.72917       0.77381       0.94444       0.63889       \n","BCD(Bray-Curtis dissimilarity)                                    0.00323       0.00161       0.00081       0.00081       0.00242       0.00161       0.00081       0.00323       0.00161       0.00242       0.00161       0.00161       0.00161       0.00081       0.00161       0.0           0.00242       0.0           0.00323       0.00081       0.00081       0.0           0.00081       0.00161       0.00242       0.00081       0.00403       0.00242       0.0           0.00403       0.0           0.00081       0.00161       0.00161       0.00161       0.00081       0.00403       0.00726       0.00565       0.0           0.0           0.00161       0.00323       0.00242       0.00323       0.00081       0.0           0.00161       0.00645       0.01048       0.00403       0.00323       0.00565       0.00242       0.00081       0.0           0.00323       0.00323       0.00323       0.00081       0.00081       0.00403       \n","BM(Informedness or bookmaker informedness)                        0.1201        0.49186       0.99837       0.85549       0.69066       0.75          0.72399       0.63636       0.8701        0.55392       0.8701        0.66015       0.81084       0.74671       0.77778       1.0           0.72727       0.89836       0.49836       0.8284        0.9           0.88725       0.99836       0.8           0.89344       0.81654       0.66667       0.9951        0.74673       0.87907       0.69508       0.77287       0.72563       0.81818       0.99674       0.62173       0.89016       0.98529       0.71085       0.92692       0.4918        0.59672       0.7045        0.9951        0.55556       0.90581       0.7745        0.85225       0.2           0.91796       0.78852       0.8918        0.46154       0.55392       0.32845       0.87337       0.92922       0.63636       0.58169       0.83008       0.88889       0.4967        \n","CEN(Confusion entropy)                                            0.27768       0.2933        0.04107       0.07623       0.11679       0.05787       0.11822       0.09559       0.1337        0.1879        0.11767       0.23543       0.07153       0.14189       0.07214       0             0.09678       0.06236       0.18036       0.12246       0.03226       0.06685       0.02838       0.06685       0.0917        0.09053       0.11092       0.08159       0.12625       0.16881       0.18708       0.12518       0.12472       0.06236       0.07214       0.14943       0.16453       0.21377       0.2669        0.04954       0.27749       0.20055       0.25137       0.09678       0.15696       0.07259       0.1337        0.12625       0.10855       0.18317       0.21377       0.1368        0.17451       0.16866       0.2914        0.07214       0.11631       0.09559       0.17265       0.12321       0.03469       0.2303        \n","DOR(Diagnostic odds ratio)                                        29.0          121.8         None          3630.0        1363.5        None          809.33333     None          1421.0        762.5         1421.0        305.0         2613.0        909.0         None          None          None          5481.0        609.0         1008.33333    None          4880.0        None          None          1363.5        2736.0        None          None          915.0         806.66667     472.11111     709.33333     1621.33333    None          None          508.33333     906.0         None          159.73333     7865.0        121.0         456.0         252.91667     None          None          3035.0        1065.75       1220.0        None          673.14286     344.57143     1089.0        None          762.5         101.83333     4277.0        1797.0        None          849.8         1530.0        None          302.0         \n","DP(Discriminant power)                                            0.80626       1.14988       None          1.96268       1.72823       None          1.60333       None          1.73812       1.58906       1.73812       1.36967       1.88397       1.63114       None          None          None          2.06134       1.53524       1.65597       None          2.03353       None          None          1.72823       1.89498       None          None          1.63272       1.60254       1.47428       1.57176       1.7697        None          None          1.49198       1.63035       None          1.2148        2.14781       1.1483        1.46596       1.32483       None          None          1.91981       1.66923       1.7016        None          1.55922       1.39888       1.6744        None          1.58906       1.10701       2.00195       1.79433       None          1.61502       1.75581       None          1.3673        \n","DPI(Discriminant power interpretation)                            Poor          Limited       None          Limited       Limited       None          Limited       None          Limited       Limited       Limited       Limited       Limited       Limited       None          None          None          Fair          Limited       Limited       None          Fair          None          None          Limited       Limited       None          None          Limited       Limited       Limited       Limited       Limited       None          None          Limited       Limited       None          Limited       Fair          Limited       Limited       Limited       None          None          Limited       Limited       Limited       None          Limited       Limited       Limited       None          Limited       Limited       Fair          Limited       None          Limited       Limited       None          Limited       \n","ERR(Error rate)                                                   0.01613       0.0129        0.00161       0.00484       0.00806       0.00323       0.00806       0.00645       0.00645       0.00806       0.00645       0.00968       0.00645       0.00806       0.00323       0.0           0.00484       0.00323       0.00968       0.00806       0.00161       0.00323       0.00161       0.00323       0.00806       0.00484       0.00806       0.00484       0.00645       0.01129       0.00968       0.00806       0.00645       0.00323       0.00323       0.00806       0.01129       0.01452       0.02097       0.00323       0.01613       0.00968       0.0129        0.00484       0.00645       0.00484       0.00645       0.00645       0.0129        0.02419       0.01452       0.00968       0.01129       0.00806       0.01129       0.00323       0.00968       0.00645       0.00968       0.00484       0.00161       0.01452       \n","F0.5(F0.5 score)                                                  0.20833       0.39474       0.88235       0.90909       0.84906       0.9375        0.78431       0.89744       0.72917       0.75758       0.72917       0.52632       0.90278       0.80357       0.94595       1.0           0.93023       0.9           0.73529       0.78125       0.97826       0.88889       0.9322        0.95238       0.72581       0.88235       0.90909       0.76923       0.75          0.61538       0.7           0.71429       0.85106       0.95745       0.81395       0.69444       0.64286       0.52632       0.48193       0.92857       0.5           0.71429       0.4902        0.76923       0.86207       0.84746       0.77778       0.69767       0.55556       0.58394       0.57143       0.68182       0.81081       0.75758       0.38462       0.875         0.78125       0.89744       0.79545       0.73529       0.97561       0.7           \n","F1(F1 score - harmonic mean of precision and sensitivity)         0.16667       0.42857       0.92308       0.88889       0.78261       0.85714       0.7619        0.77778       0.77778       0.66667       0.77778       0.57143       0.86667       0.78261       0.875         1.0           0.84211       0.9           0.625         0.8           0.94737       0.88889       0.95652       0.88889       0.78261       0.85714       0.8           0.84211       0.75          0.69565       0.7           0.73684       0.8           0.9           0.875         0.66667       0.72          0.64          0.55172       0.92857       0.5           0.66667       0.55556       0.84211       0.71429       0.86957       0.77778       0.75          0.33333       0.68085       0.64          0.75          0.63158       0.66667       0.36364       0.875         0.83333       0.77778       0.7           0.76923       0.94118       0.6087        \n","F2(F2 score)                                                      0.13889       0.46875       0.96774       0.86957       0.72581       0.78947       0.74074       0.68627       0.83333       0.59524       0.83333       0.625         0.83333       0.76271       0.81395       1.0           0.76923       0.9           0.54348       0.81967       0.91837       0.88889       0.98214       0.83333       0.84906       0.83333       0.71429       0.93023       0.75          0.8           0.7           0.76087       0.75472       0.84906       0.94595       0.64103       0.81818       0.81633       0.64516       0.92857       0.5           0.625         0.64103       0.93023       0.60976       0.89286       0.77778       0.81081       0.2381        0.81633       0.72727       0.83333       0.51724       0.59524       0.34483       0.875         0.89286       0.68627       0.625         0.80645       0.90909       0.53846       \n","FDR(False discovery rate)                                         0.75          0.625         0.14286       0.07692       0.1           0.0           0.2           0.0           0.3           0.16667       0.3           0.5           0.07143       0.18182       0.0           0.0           0.0           0.1           0.16667       0.23077       0.0           0.11111       0.08333       0.0           0.30769       0.1           0.0           0.27273       0.25          0.42857       0.3           0.3           0.11111       0.0           0.22222       0.28571       0.4           0.52941       0.55556       0.07143       0.5           0.25          0.54545       0.27273       0.0           0.16667       0.22222       0.33333       0.0           0.46667       0.46667       0.35714       0.0           0.16667       0.6           0.125         0.25          0.0           0.125         0.28571       0.0           0.22222       \n","FN(False negative/miss/type 2 error)                              7             3             0             2             4             2             3             4             1             4             1             2             3             3             2             0             3             1             5             2             1             1             0             2             1             2             5             0             2             1             3             2             3             2             0             3             1             0             3             1             5             4             2             0             4             1             2             1             8             1             2             1             7             4             4             1             1             4             5             1             1             7             \n","FNR(Miss rate or false negative rate)                             0.875         0.5           0.0           0.14286       0.30769       0.25          0.27273       0.36364       0.125         0.44444       0.125         0.33333       0.1875        0.25          0.22222       0.0           0.27273       0.1           0.5           0.16667       0.1           0.11111       0.0           0.2           0.1           0.18182       0.33333       0.0           0.25          0.11111       0.3           0.22222       0.27273       0.18182       0.0           0.375         0.1           0.0           0.27273       0.07143       0.5           0.4           0.28571       0.0           0.44444       0.09091       0.22222       0.14286       0.8           0.05882       0.2           0.1           0.53846       0.44444       0.66667       0.125         0.0625        0.36364       0.41667       0.16667       0.11111       0.5           \n","FOR(False omission rate)                                          0.01136       0.0049        0.0           0.00329       0.00656       0.00326       0.00492       0.00653       0.00164       0.00651       0.00164       0.00327       0.00495       0.00493       0.00326       0.0           0.0049        0.00164       0.00814       0.00329       0.00164       0.00164       0.0           0.00327       0.00165       0.00328       0.0082        0.0           0.00327       0.00165       0.00492       0.00328       0.00491       0.00327       0.0           0.00489       0.00165       0.0           0.00498       0.00165       0.0082        0.00654       0.00328       0.0           0.0065        0.00164       0.00327       0.00164       0.01294       0.00169       0.00331       0.00165       0.0114        0.00651       0.0065        0.00163       0.00167       0.00653       0.00817       0.00163       0.00163       0.01146       \n","FP(False positive/type 1 error/false alarm)                       3             5             1             1             1             0             2             0             3             1             3             4             1             2             0             0             0             1             1             3             0             1             1             0             4             1             0             3             2             6             3             3             1             0             2             2             6             9             10            1             5             2             6             3             0             2             2             3             0             14            7             5             0             1             3             1             5             0             1             2             0             2             \n","FPR(Fall-out or false positive rate)                              0.0049        0.00814       0.00163       0.00165       0.00165       0.0           0.00328       0.0           0.0049        0.00164       0.0049        0.00651       0.00166       0.00329       0.0           0.0           0.0           0.00164       0.00164       0.00493       0.0           0.00164       0.00164       0.0           0.00656       0.00164       0.0           0.0049        0.00327       0.00982       0.00492       0.00491       0.00164       0.0           0.00326       0.00327       0.00984       0.01471       0.01642       0.00165       0.0082        0.00328       0.00979       0.0049        0.0           0.00328       0.00327       0.00489       0.0           0.02322       0.01148       0.0082        0.0           0.00164       0.00489       0.00163       0.00828       0.0           0.00164       0.00326       0.0           0.0033        \n","G(G-measure geometric mean of precision and sensitivity)          0.17678       0.43301       0.92582       0.8895        0.78935       0.86603       0.76277       0.79772       0.78262       0.68041       0.78262       0.57735       0.8686        0.78335       0.88192       1.0           0.8528        0.9           0.6455        0.80064       0.94868       0.88889       0.95743       0.89443       0.78935       0.85812       0.8165        0.8528        0.75          0.7127        0.7           0.73786       0.80403       0.90453       0.88192       0.66815       0.73485       0.68599       0.56854       0.92857       0.5           0.67082       0.5698        0.8528        0.74536       0.87039       0.77778       0.75593       0.44721       0.70849       0.6532        0.76064       0.67937       0.68041       0.36515       0.875         0.83853       0.79772       0.71443       0.77152       0.94281       0.62361       \n","GI(Gini index)                                                    0.1201        0.49186       0.99837       0.85549       0.69066       0.75          0.72399       0.63636       0.8701        0.55392       0.8701        0.66015       0.81084       0.74671       0.77778       1.0           0.72727       0.89836       0.49836       0.8284        0.9           0.88725       0.99836       0.8           0.89344       0.81654       0.66667       0.9951        0.74673       0.87907       0.69508       0.77287       0.72563       0.81818       0.99674       0.62173       0.89016       0.98529       0.71085       0.92692       0.4918        0.59672       0.7045        0.9951        0.55556       0.90581       0.7745        0.85225       0.2           0.91796       0.78852       0.8918        0.46154       0.55392       0.32845       0.87337       0.92922       0.63636       0.58169       0.83008       0.88889       0.4967        \n","GM(G-mean geometric mean of specificity and sensitivity)          0.35269       0.70422       0.99919       0.92506       0.83136       0.86603       0.8514        0.79772       0.93312       0.74475       0.93312       0.81383       0.90064       0.8646        0.88192       1.0           0.8528        0.94791       0.70653       0.91062       0.94868       0.94204       0.99918       0.89443       0.94557       0.90379       0.8165        0.99755       0.86461       0.93817       0.8346        0.87975       0.8521        0.90453       0.99837       0.78928       0.94401       0.99262       0.84577       0.96283       0.7042        0.77333       0.84101       0.99755       0.74536       0.9519        0.88047       0.92355       0.44721       0.95881       0.88928       0.94479       0.67937       0.74475       0.57594       0.93465       0.96423       0.79772       0.76313       0.91138       0.94281       0.70594       \n","IBA(Index of balanced accuracy)                                   0.01616       0.252         1.0           0.73489       0.47964       0.5625        0.52957       0.40496       0.76614       0.30904       0.76614       0.44586       0.66041       0.56311       0.60494       1.0           0.52893       0.81015       0.25041       0.69511       0.81          0.79028       1.0           0.64          0.81055       0.66966       0.44444       0.99998       0.5631        0.79101       0.49102       0.60577       0.52925       0.66942       0.99999       0.39138       0.8108        0.99978       0.53199       0.86235       0.25202       0.36078       0.51213       0.99998       0.30864       0.82671       0.6055        0.73527       0.04          0.88659       0.64173       0.81068       0.21302       0.30904       0.11219       0.7658        0.87933       0.40496       0.34068       0.69489       0.79012       0.25082       \n","ICSI(Individual classification success index)                     -0.625        -0.125        0.85714       0.78022       0.59231       0.75          0.52727       0.63636       0.575         0.38889       0.575         0.16667       0.74107       0.56818       0.77778       1.0           0.72727       0.8           0.33333       0.60256       0.9           0.77778       0.91667       0.8           0.59231       0.71818       0.66667       0.72727       0.5           0.46032       0.4           0.47778       0.61616       0.81818       0.77778       0.33929       0.5           0.47059       0.17172       0.85714       0.0           0.35          0.16883       0.72727       0.55556       0.74242       0.55556       0.52381       0.2           0.47451       0.33333       0.54286       0.46154       0.38889       -0.26667      0.75          0.6875        0.63636       0.45833       0.54762       0.88889       0.27778       \n","IS(Information score)                                             4.27612       5.27612       6.46877       5.35329       5.42368       6.27612       5.49476       5.81669       5.76155       5.84316       5.76155       5.69116       5.16921       5.40166       6.1062        5.9542        5.81669       5.80219       5.69116       5.31265       5.9542        5.93627       5.69116       5.9542        5.42368       5.66469       5.36923       5.81669       5.86109       5.29884       5.43962       5.59163       5.64677       5.81669       6.1062        5.7907        5.21723       5.18866       4.64677       5.36185       4.9542        5.53916       5.33127       5.81669       6.1062        5.55366       5.74363       5.88381       5.9542        4.28177       5.04731       5.31677       5.57568       5.84316       5.36923       6.08348       4.86109       5.81669       5.49852       6.20574       6.1062        5.1062        \n","J(Jaccard index)                                                  0.09091       0.27273       0.85714       0.8           0.64286       0.75          0.61538       0.63636       0.63636       0.5           0.63636       0.4           0.76471       0.64286       0.77778       1.0           0.72727       0.81818       0.45455       0.66667       0.9           0.8           0.91667       0.8           0.64286       0.75          0.66667       0.72727       0.6           0.53333       0.53846       0.58333       0.66667       0.81818       0.77778       0.5           0.5625        0.47059       0.38095       0.86667       0.33333       0.5           0.38462       0.72727       0.55556       0.76923       0.63636       0.6           0.2           0.51613       0.47059       0.6           0.46154       0.5           0.22222       0.77778       0.71429       0.63636       0.53846       0.625         0.88889       0.4375        \n","LS(Lift score)                                                    19.375        38.75         88.57143      40.87912      42.92308      77.5          45.09091      56.36364      54.25         57.40741      54.25         51.66667      35.98214      42.27273      68.88889      62.0          56.36364      55.8          51.66667      39.74359      62.0          61.23457      51.66667      62.0          42.92308      50.72727      41.33333      56.36364      58.125        39.36508      43.4          48.22222      50.10101      56.36364      68.88889      55.35714      37.2          36.47059      25.05051      41.12245      31.0          46.5          40.25974      56.36364      68.88889      46.9697       53.58025      59.04762      62.0          19.45098      33.06667      39.85714      47.69231      57.40741      41.33333      67.8125       29.0625       56.36364      45.20833      73.80952      68.88889      34.44444      \n","MCC(Matthews correlation coefficient)                             0.16929       0.42666       0.92507       0.88706       0.78554       0.86461       0.7587        0.79512       0.77951       0.67675       0.77951       0.57264       0.8654        0.77927       0.88048       1.0           0.85071       0.89836       0.64128       0.79656       0.94791       0.88725       0.95664       0.89296       0.78554       0.85569       0.81314       0.85071       0.74673       0.70772       0.69508       0.73381       0.8009        0.90305       0.88048       0.66412       0.72981       0.68093       0.55892       0.92692       0.4918        0.66606       0.56384       0.85071       0.74293       0.86796       0.7745        0.75284       0.44431       0.69859       0.64648       0.75619       0.67548       0.67675       0.3595        0.87337       0.83389       0.79512       0.71009       0.76913       0.94204       0.61695       \n","MCCI(Matthews correlation coefficient interpretation)             Negligible    Weak          Very Strong   Strong        Strong        Strong        Strong        Strong        Strong        Moderate      Strong        Moderate      Strong        Strong        Strong        Very Strong   Strong        Strong        Moderate      Strong        Very Strong   Strong        Very Strong   Strong        Strong        Strong        Strong        Strong        Strong        Strong        Moderate      Strong        Strong        Very Strong   Strong        Moderate      Strong        Moderate      Moderate      Very Strong   Weak          Moderate      Moderate      Strong        Strong        Strong        Strong        Strong        Weak          Moderate      Moderate      Strong        Moderate      Moderate      Weak          Strong        Strong        Strong        Strong        Strong        Very Strong   Moderate      \n","MCEN(Modified confusion entropy)                                  0.28646       0.33678       0.05787       0.11274       0.15497       0.07214       0.15258       0.11914       0.18151       0.23965       0.15527       0.28758       0.09841       0.19619       0.10164       0             0.13613       0.09075       0.21979       0.16866       0.04793       0.09586       0.0431        0.09586       0.11375       0.12931       0.14943       0.1099        0.16286       0.21733       0.24642       0.15835       0.17242       0.09075       0.10164       0.18194       0.21643       0.27187       0.32698       0.07516       0.33007       0.25863       0.30637       0.13613       0.20328       0.10101       0.18151       0.16286       0.0999        0.2358        0.27187       0.17974       0.21252       0.21079       0.32367       0.10164       0.16733       0.11914       0.22423       0.16232       0.05082       0.28857       \n","MK(Markedness)                                                    0.23864       0.3701        0.85714       0.91978       0.89344       0.99674       0.79508       0.99347       0.69836       0.82682       0.69836       0.49673       0.92362       0.81326       0.99674       1.0           0.9951        0.89836       0.82519       0.76594       0.99836       0.88725       0.91667       0.99673       0.69066       0.89672       0.9918        0.72727       0.74673       0.56978       0.69508       0.69672       0.88398       0.99673       0.77778       0.70939       0.59835       0.47059       0.43946       0.92692       0.4918        0.74346       0.45126       0.72727       0.9935        0.83169       0.7745        0.66503       0.98706       0.53164       0.53003       0.64121       0.9886        0.82682       0.3935        0.87337       0.74833       0.99347       0.86683       0.71265       0.99837       0.76632       \n","N(Condition negative)                                             612           614           614           606           607           612           609           609           612           611           612           614           604           608           611           610           609           610           610           608           610           611           609           610           610           609           605           612           612           611           610           611           609           609           613           612           610           612           609           606           610           610           613           612           611           609           611           613           610           603           610           610           607           611           614           612           604           609           608           614           611           606           \n","NLR(Negative likelihood ratio)                                    0.87931       0.50411       0.0           0.14309       0.3082        0.25          0.27363       0.36364       0.12562       0.44517       0.12562       0.33552       0.18781       0.25083       0.22222       0.0           0.27273       0.10016       0.50082       0.16749       0.1           0.11129       0.0           0.2           0.10066       0.18212       0.33333       0.0           0.25082       0.11221       0.30148       0.22332       0.27318       0.18182       0.0           0.37623       0.10099       0.0           0.27728       0.07155       0.50413       0.40132       0.28854       0.0           0.44444       0.09121       0.22295       0.14356       0.8           0.06022       0.20232       0.10083       0.53846       0.44517       0.66994       0.1252        0.06302       0.36364       0.41735       0.16721       0.11111       0.50166       \n","NLRI(Negative likelihood ratio interpretation)                    Negligible    Negligible    Good          Fair          Poor          Poor          Poor          Poor          Fair          Poor          Fair          Poor          Fair          Poor          Poor          Good          Poor          Fair          Negligible    Fair          Good          Fair          Good          Fair          Fair          Fair          Poor          Good          Poor          Fair          Poor          Poor          Poor          Fair          Good          Poor          Fair          Good          Poor          Good          Negligible    Poor          Poor          Good          Poor          Good          Poor          Fair          Negligible    Good          Poor          Fair          Negligible    Poor          Negligible    Fair          Good          Poor          Poor          Fair          Fair          Negligible    \n","NPV(Negative predictive value)                                    0.98864       0.9951        1.0           0.99671       0.99344       0.99674       0.99508       0.99347       0.99836       0.99349       0.99836       0.99673       0.99505       0.99507       0.99674       1.0           0.9951        0.99836       0.99186       0.99671       0.99836       0.99836       1.0           0.99673       0.99835       0.99672       0.9918        1.0           0.99673       0.99835       0.99508       0.99672       0.99509       0.99673       1.0           0.99511       0.99835       1.0           0.99502       0.99835       0.9918        0.99346       0.99672       1.0           0.9935        0.99836       0.99673       0.99836       0.98706       0.99831       0.99669       0.99835       0.9886        0.99349       0.9935        0.99837       0.99833       0.99347       0.99183       0.99837       0.99837       0.98854       \n","OC(Overlap coefficient)                                           0.25          0.5           1.0           0.92308       0.9           1.0           0.8           1.0           0.875         0.83333       0.875         0.66667       0.92857       0.81818       1.0           1.0           1.0           0.9           0.83333       0.83333       1.0           0.88889       1.0           1.0           0.9           0.9           1.0           1.0           0.75          0.88889       0.7           0.77778       0.88889       1.0           1.0           0.71429       0.9           1.0           0.72727       0.92857       0.5           0.75          0.71429       1.0           1.0           0.90909       0.77778       0.85714       1.0           0.94118       0.8           0.9           1.0           0.83333       0.4           0.875         0.9375        1.0           0.875         0.83333       1.0           0.77778       \n","OOC(Otsuka-Ochiai coefficient)                                    0.17678       0.43301       0.92582       0.8895        0.78935       0.86603       0.76277       0.79772       0.78262       0.68041       0.78262       0.57735       0.8686        0.78335       0.88192       1.0           0.8528        0.9           0.6455        0.80064       0.94868       0.88889       0.95743       0.89443       0.78935       0.85812       0.8165        0.8528        0.75          0.7127        0.7           0.73786       0.80403       0.90453       0.88192       0.66815       0.73485       0.68599       0.56854       0.92857       0.5           0.67082       0.5698        0.8528        0.74536       0.87039       0.77778       0.75593       0.44721       0.70849       0.6532        0.76064       0.67937       0.68041       0.36515       0.875         0.83853       0.79772       0.71443       0.77152       0.94281       0.62361       \n","OP(Optimized precision)                                           0.20707       0.6574        0.99757       0.91906       0.81091       0.85392       0.83564       0.77133       0.92933       0.70697       0.92933       0.79346       0.89092       0.85069       0.87177       1.0           0.83727       0.94496       0.65772       0.90348       0.94576       0.93877       0.99757       0.88566       0.94258       0.89597       0.79194       0.9927        0.85229       0.9348        0.81624       0.86936       0.83645       0.89677       0.99514       0.76272       0.94101       0.97808       0.82922       0.96056       0.6542        0.74186       0.82522       0.9927        0.70783       0.94918       0.87016       0.91906       0.32043       0.95724       0.88008       0.9418        0.62029       0.70697       0.49055       0.93092       0.96222       0.77133       0.72793       0.90587       0.93956       0.65362       \n","P(Condition positive or support)                                  8             6             6             14            13            8             11            11            8             9             8             6             16            12            9             10            11            10            10            12            10            9             11            10            10            11            15            8             8             9             10            9             11            11            7             8             10            8             11            14            10            10            7             8             9             11            9             7             10            17            10            10            13            9             6             8             16            11            12            6             9             14            \n","PLR(Positive likelihood ratio)                                    25.5          61.4          614.0         519.42857     420.23077     None          221.45455     None          178.5         339.44444     178.5         102.33333     490.75        228.0         None          None          None          549.0         305.0         168.88889     None          543.11111     609.0         None          137.25        498.27273     None          204.0         229.5         90.51852      142.33333     158.40741     442.90909     None          306.5         191.25        91.5          68.0          44.29091      562.71429     61.0          183.0         72.97619      204.0         None          276.81818     237.61111     175.14286     None          40.53782      69.71429      109.8         None          339.44444     68.22222      535.5         113.25        None          354.66667     255.83333     None          151.5         \n","PLRI(Positive likelihood ratio interpretation)                    Good          Good          Good          Good          Good          None          Good          None          Good          Good          Good          Good          Good          Good          None          None          None          Good          Good          Good          None          Good          Good          None          Good          Good          None          Good          Good          Good          Good          Good          Good          None          Good          Good          Good          Good          Good          Good          Good          Good          Good          Good          None          Good          Good          Good          None          Good          Good          Good          None          Good          Good          Good          Good          None          Good          Good          None          Good          \n","POP(Population)                                                   620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           \n","PPV(Precision or positive predictive value)                       0.25          0.375         0.85714       0.92308       0.9           1.0           0.8           1.0           0.7           0.83333       0.7           0.5           0.92857       0.81818       1.0           1.0           1.0           0.9           0.83333       0.76923       1.0           0.88889       0.91667       1.0           0.69231       0.9           1.0           0.72727       0.75          0.57143       0.7           0.7           0.88889       1.0           0.77778       0.71429       0.6           0.47059       0.44444       0.92857       0.5           0.75          0.45455       0.72727       1.0           0.83333       0.77778       0.66667       1.0           0.53333       0.53333       0.64286       1.0           0.83333       0.4           0.875         0.75          1.0           0.875         0.71429       1.0           0.77778       \n","PRE(Prevalence)                                                   0.0129        0.00968       0.00968       0.02258       0.02097       0.0129        0.01774       0.01774       0.0129        0.01452       0.0129        0.00968       0.02581       0.01935       0.01452       0.01613       0.01774       0.01613       0.01613       0.01935       0.01613       0.01452       0.01774       0.01613       0.01613       0.01774       0.02419       0.0129        0.0129        0.01452       0.01613       0.01452       0.01774       0.01774       0.01129       0.0129        0.01613       0.0129        0.01774       0.02258       0.01613       0.01613       0.01129       0.0129        0.01452       0.01774       0.01452       0.01129       0.01613       0.02742       0.01613       0.01613       0.02097       0.01452       0.00968       0.0129        0.02581       0.01774       0.01935       0.00968       0.01452       0.02258       \n","Q(Yule Q - coefficient of colligation)                            0.93333       0.98371       None          0.99945       0.99853       None          0.99753       None          0.99859       0.99738       0.99859       0.99346       0.99923       0.9978        None          None          None          0.99964       0.99672       0.99802       None          0.99959       None          None          0.99853       0.99927       None          None          0.99782       0.99752       0.99577       0.99718       0.99877       None          None          0.99607       0.99779       None          0.98756       0.99975       0.98361       0.99562       0.99212       None          None          0.99934       0.99813       0.99836       None          0.99703       0.99421       0.99817       None          0.99738       0.98055       0.99953       0.99889       None          0.99765       0.99869       None          0.9934        \n","QI(Yule Q interpretation)                                         Strong        Strong        None          Strong        Strong        None          Strong        None          Strong        Strong        Strong        Strong        Strong        Strong        None          None          None          Strong        Strong        Strong        None          Strong        None          None          Strong        Strong        None          None          Strong        Strong        Strong        Strong        Strong        None          None          Strong        Strong        None          Strong        Strong        Strong        Strong        Strong        None          None          Strong        Strong        Strong        None          Strong        Strong        Strong        None          Strong        Strong        Strong        Strong        None          Strong        Strong        None          Strong        \n","RACC(Random accuracy)                                             8e-05         0.00012       0.00011       0.00047       0.00034       0.00012       0.00029       0.0002        0.00021       0.00014       0.00021       0.00012       0.00058       0.00034       0.00016       0.00026       0.00023       0.00026       0.00016       0.00041       0.00023       0.00021       0.00034       0.00021       0.00034       0.00029       0.00039       0.00023       0.00017       0.00033       0.00026       0.00023       0.00026       0.00026       0.00016       0.00015       0.00039       0.00035       0.00052       0.00051       0.00026       0.00021       0.0002        0.00023       0.00012       0.00034       0.00021       0.00016       5e-05         0.00133       0.00039       0.00036       0.0002        0.00014       8e-05         0.00017       0.00083       0.0002        0.00025       0.00011       0.00019       0.00033       \n","RACCU(Random accuracy unbiased)                                   9e-05         0.00013       0.00011       0.00047       0.00034       0.00013       0.00029       0.00021       0.00021       0.00015       0.00021       0.00013       0.00059       0.00034       0.00017       0.00026       0.00023       0.00026       0.00017       0.00041       0.00023       0.00021       0.00034       0.00021       0.00034       0.00029       0.00041       0.00023       0.00017       0.00034       0.00026       0.00023       0.00026       0.00026       0.00017       0.00015       0.00041       0.00041       0.00055       0.00051       0.00026       0.00021       0.00021       0.00023       0.00013       0.00034       0.00021       0.00017       9e-05         0.00144       0.00041       0.00037       0.00023       0.00015       8e-05         0.00017       0.00084       0.00021       0.00026       0.00011       0.00019       0.00034       \n","TN(True negative/correct rejection)                               609           609           613           605           606           612           607           609           609           610           609           610           603           606           611           610           609           609           609           605           610           610           608           610           606           608           605           609           610           605           607           608           608           609           611           610           604           603           599           605           605           608           607           609           611           607           609           610           610           589           603           605           607           610           611           611           599           609           607           612           611           604           \n","TNR(Specificity or true negative rate)                            0.9951        0.99186       0.99837       0.99835       0.99835       1.0           0.99672       1.0           0.9951        0.99836       0.9951        0.99349       0.99834       0.99671       1.0           1.0           1.0           0.99836       0.99836       0.99507       1.0           0.99836       0.99836       1.0           0.99344       0.99836       1.0           0.9951        0.99673       0.99018       0.99508       0.99509       0.99836       1.0           0.99674       0.99673       0.99016       0.98529       0.98358       0.99835       0.9918        0.99672       0.99021       0.9951        1.0           0.99672       0.99673       0.99511       1.0           0.97678       0.98852       0.9918        1.0           0.99836       0.99511       0.99837       0.99172       1.0           0.99836       0.99674       1.0           0.9967        \n","TON(Test outcome negative)                                        616           612           613           607           610           614           610           613           610           614           610           612           606           609           613           610           612           610           614           607           611           611           608           612           607           610           610           609           612           606           610           610           611           611           611           613           605           603           602           606           610           612           609           609           615           608           611           611           618           590           605           606           614           614           615           612           600           613           612           613           612           611           \n","TOP(Test outcome positive)                                        4             8             7             13            10            6             10            7             10            6             10            8             14            11            7             10            8             10            6             13            9             9             12            8             13            10            10            11            8             14            10            10            9             9             9             7             15            17            18            14            10            8             11            11            5             12            9             9             2             30            15            14            6             6             5             8             20            7             8             7             8             9             \n","TP(True positive/hit)                                             1             3             6             12            9             6             8             7             7             5             7             4             13            9             7             10            8             9             5             10            9             8             11            8             9             9             10            8             6             8             7             7             8             9             7             5             9             8             8             13            5             6             5             8             5             10            7             6             2             16            8             9             6             5             2             7             15            7             7             5             8             7             \n","TPR(Sensitivity, recall, hit rate, or true positive rate)         0.125         0.5           1.0           0.85714       0.69231       0.75          0.72727       0.63636       0.875         0.55556       0.875         0.66667       0.8125        0.75          0.77778       1.0           0.72727       0.9           0.5           0.83333       0.9           0.88889       1.0           0.8           0.9           0.81818       0.66667       1.0           0.75          0.88889       0.7           0.77778       0.72727       0.81818       1.0           0.625         0.9           1.0           0.72727       0.92857       0.5           0.6           0.71429       1.0           0.55556       0.90909       0.77778       0.85714       0.2           0.94118       0.8           0.9           0.46154       0.55556       0.33333       0.875         0.9375        0.63636       0.58333       0.83333       0.88889       0.5           \n","Y(Youden index)                                                   0.1201        0.49186       0.99837       0.85549       0.69066       0.75          0.72399       0.63636       0.8701        0.55392       0.8701        0.66015       0.81084       0.74671       0.77778       1.0           0.72727       0.89836       0.49836       0.8284        0.9           0.88725       0.99836       0.8           0.89344       0.81654       0.66667       0.9951        0.74673       0.87907       0.69508       0.77287       0.72563       0.81818       0.99674       0.62173       0.89016       0.98529       0.71085       0.92692       0.4918        0.59672       0.7045        0.9951        0.55556       0.90581       0.7745        0.85225       0.2           0.91796       0.78852       0.8918        0.46154       0.55392       0.32845       0.87337       0.92922       0.63636       0.58169       0.83008       0.88889       0.4967        \n","dInd(Distance index)                                              0.87501       0.50007       0.00163       0.14287       0.3077        0.25          0.27275       0.36364       0.1251        0.44445       0.1251        0.3334        0.18751       0.25002       0.22222       0.0           0.27273       0.10001       0.5           0.16674       0.1           0.11112       0.00164       0.2           0.10021       0.18183       0.33333       0.0049        0.25002       0.11154       0.30004       0.22228       0.27273       0.18182       0.00326       0.37501       0.10048       0.01471       0.27322       0.07145       0.50007       0.40001       0.28588       0.0049        0.44444       0.09097       0.22225       0.14294       0.8           0.06324       0.20033       0.10034       0.53846       0.44445       0.66668       0.12501       0.06305       0.36364       0.41667       0.1667        0.11111       0.50001       \n","sInd(Similarity index)                                            0.38127       0.6464        0.99885       0.89898       0.78243       0.82322       0.80714       0.74287       0.91154       0.68573       0.91154       0.76425       0.86741       0.82321       0.84287       1.0           0.80715       0.92928       0.64644       0.8821        0.92929       0.92142       0.99884       0.85858       0.92914       0.87143       0.7643        0.99653       0.82321       0.92113       0.78784       0.84283       0.80715       0.87144       0.99769       0.73482       0.92895       0.9896        0.8068        0.94948       0.6464        0.71715       0.79785       0.99653       0.68573       0.93568       0.84285       0.89893       0.43431       0.95528       0.85835       0.92905       0.61925       0.68573       0.52858       0.9116        0.95542       0.74287       0.70537       0.88213       0.92143       0.64644       \n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pycm/pycm_obj.py:195: RuntimeWarning: The confusion matrix is a high dimension matrix and won't be demonstrated properly.\n","If confusion matrix has too many zeros (sparse matrix) you can set `sparse` flag to True in printing functions otherwise by using save_csv method to save the confusion matrix in csv format you'll have better demonstration.\n","  warn(CLASS_NUMBER_WARNING, RuntimeWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jj5XcWGDb-v6","executionInfo":{"status":"ok","timestamp":1617800991805,"user_tz":-330,"elapsed":4454,"user":{"displayName":"Saloni Parekh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRkAeLdguHD8uTdcYmYuJQbGjVOo7H6xoSpvg1w=s64","userId":"13474740441446535544"}},"outputId":"3c6d31ab-25d1-433f-e91f-73d2ae1208b1"},"source":["cm2 = ConfusionMatrix(actual_vector=y2, predict_vector=y_pred2)\n","print(cm2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predict  0        1        2        3        4        5        6        7        8        9        10       11       12       13       14       15       16       17       18       19       20       21       22       23       24       25       26       27       28       29       30       31       32       33       34       35       36       37       38       39       40       41       42       43       44       45       46       47       48       49       50       51       52       53       54       55       56       57       58       59       60       61       \n","Actual\n","0        4        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        \n","\n","1        0        4        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        \n","\n","2        0        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        \n","\n","3        0        0        0        11       0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","4        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        \n","\n","5        0        0        0        0        0        4        1        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","6        0        0        0        0        0        0        11       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","7        0        0        0        0        0        0        0        8        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","8        0        0        0        0        0        0        0        0        7        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","9        0        2        0        1        0        0        0        0        0        2        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        \n","\n","10       0        0        0        0        0        0        0        0        0        0        6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        \n","\n","11       0        0        0        0        0        0        0        0        0        0        0        4        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        \n","\n","12       0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        1        0        0        0        0        0        0        0        0        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","13       0        0        0        1        0        0        0        0        0        0        0        2        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        \n","\n","14       0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","15       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        10       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","16       1        0        0        0        0        0        4        0        0        1        0        0        0        0        0        0        5        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","17       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","18       0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        4        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","19       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        \n","\n","20       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","21       0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","22       0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        1        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","23       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","24       6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        \n","\n","25       0        0        0        0        0        0        0        1        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","26       0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        13       0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","27       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","28       0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        5        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        \n","\n","29       0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","30       0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","31       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        1        0        0        0        0        \n","\n","32       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        10       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        \n","\n","33       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        \n","\n","34       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","35       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        \n","\n","36       0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        1        0        0        0        1        0        0        \n","\n","37       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","38       0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        0        1        0        0        0        0        0        0        0        0        0        1        0        0        0        1        0        0        0        0        0        0        0        \n","\n","39       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        14       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","40       0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        6        0        0        0        0        0        0        0        0        0        1        0        0        0        1        0        0        0        0        0        0        0        \n","\n","41       0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        \n","\n","42       1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","43       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        8        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","44       0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        5        0        1        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        \n","\n","45       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        1        6        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","46       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        7        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","47       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        6        0        0        0        0        0        0        0        0        0        0        0        0        0        0        \n","\n","48       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        1        0        0        \n","\n","49       0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        1        0        0        0        0        6        6        0        0        0        0        0        0        0        0        0        1        0        0        \n","\n","50       1        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        7        0        0        0        0        0        0        0        0        0        0        0        \n","\n","51       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        9        0        0        0        0        0        0        0        0        0        0        \n","\n","52       0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        1        0        0        0        0        0        0        0        0        2        6        0        0        0        1        0        0        0        0        0        \n","\n","53       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        5        0        0        0        0        0        2        0        0        \n","\n","54       0        0        0        0        0        0        1        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        \n","\n","55       0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        7        0        0        0        0        0        0        \n","\n","56       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        14       0        2        0        0        0        \n","\n","57       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        7        1        0        0        0        \n","\n","58       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        8        0        0        0        \n","\n","59       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        5        0        0        \n","\n","60       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        6        1        \n","\n","61       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        3        0        0        0        0        1        0        1        0        0        0        0        0        0        0        0        0        0        0        1        0        0        0        0        0        0        7        \n","\n","\n","\n","\n","\n","Overall Statistics : \n","\n","95% CI                                                            (0.68064,0.75162)\n","ACC Macro                                                         0.99084\n","ARI                                                               0.53048\n","AUNP                                                              0.8559\n","AUNU                                                              0.85789\n","Bangdiwala B                                                      0.56645\n","Bennett S                                                         0.71148\n","CBA                                                               0.63381\n","CSI                                                               0.46447\n","Chi-Squared                                                       22062.10853\n","Chi-Squared DF                                                    3721\n","Conditional Entropy                                               1.09062\n","Cramer V                                                          0.76377\n","Cross Entropy                                                     6.00696\n","F1 Macro                                                          0.70997\n","F1 Micro                                                          0.71613\n","FNR Macro                                                         0.27956\n","FNR Micro                                                         0.28387\n","FPR Macro                                                         0.00465\n","FPR Micro                                                         0.00465\n","Gwet AC1                                                          0.71148\n","Hamming Loss                                                      0.28387\n","Joint Entropy                                                     7.00074\n","KL Divergence                                                     0.09684\n","Kappa                                                             0.71139\n","Kappa 95% CI                                                      (0.67531,0.74747)\n","Kappa No Prevalence                                               0.43226\n","Kappa Standard Error                                              0.01841\n","Kappa Unbiased                                                    0.71123\n","Krippendorff Alpha                                                0.71147\n","Lambda A                                                          0.7131\n","Lambda B                                                          0.71285\n","Mutual Information                                                4.78801\n","NIR                                                               0.02742\n","Overall ACC                                                       0.71613\n","Overall CEN                                                       0.15197\n","Overall J                                                         (35.77327,0.57699)\n","Overall MCC                                                       0.71216\n","Overall MCEN                                                      0.2021\n","Overall RACC                                                      0.01642\n","Overall RACCU                                                     0.01695\n","P-Value                                                           0.0\n","PPV Macro                                                         0.74403\n","PPV Micro                                                         0.71613\n","Pearson C                                                         0.98624\n","Phi-Squared                                                       35.58405\n","RCI                                                               0.81014\n","RR                                                                10.0\n","Reference Entropy                                                 5.91012\n","Response Entropy                                                  5.87863\n","SOA1(Landis & Koch)                                               Substantial\n","SOA2(Fleiss)                                                      Intermediate to Good\n","SOA3(Altman)                                                      Good\n","SOA4(Cicchetti)                                                   Good\n","SOA5(Cramer)                                                      Strong\n","SOA6(Matthews)                                                    Strong\n","Scott PI                                                          0.71123\n","Standard Error                                                    0.01811\n","TNR Macro                                                         0.99535\n","TNR Micro                                                         0.99535\n","TPR Macro                                                         0.72044\n","TPR Micro                                                         0.71613\n","Zero-one Loss                                                     176\n","\n","Class Statistics :\n","\n","Classes                                                           0             1             2             3             4             5             6             7             8             9             10            11            12            13            14            15            16            17            18            19            20            21            22            23            24            25            26            27            28            29            30            31            32            33            34            35            36            37            38            39            40            41            42            43            44            45            46            47            48            49            50            51            52            53            54            55            56            57            58            59            60            61            \n","ACC(Accuracy)                                                     0.97903       0.98387       0.99516       0.99194       0.99032       0.99355       0.98387       0.99355       0.99355       0.98065       0.99677       0.98548       0.9871        0.99194       0.99194       0.99839       0.99032       0.99516       0.9871        0.99194       0.99516       0.99839       0.99355       0.99839       0.97742       0.99516       0.99355       0.99839       0.99032       0.99355       0.99839       0.99355       0.99194       0.99677       0.99839       0.98226       0.98871       0.99839       0.9871        1.0           0.99032       0.9871        0.9871        0.99677       0.99194       0.99032       0.99032       0.99516       0.98871       0.98226       0.98548       0.99194       0.9871        0.99194       0.98387       0.99032       0.99194       0.99194       0.98548       0.98548       0.99516       0.98065       \n","AGF(Adjusted F-score)                                             0.66392       0.7434        0.7439        0.89074       0.80698       0.74341       0.91833       0.86689       0.91182       0.47964       0.88736       0.75407       0.77609       0.83537       0.8707        0.98998       0.71149       0.90218       0.65658       0.87132       0.90218       0.95284       0.90305       0.95769       0.45359       0.91152       0.92941       0.98757       0.78863       0.92147       0.95769       0.88047       0.92725       0.92023       0.98585       0.79669       0.78002       0.98757       0.7951        1.0           0.78824       0.85867       0.85385       0.97558       0.76938       0.76433       0.86125       0.91198       0.90304       0.63212       0.81023       0.92023       0.70969       0.76938       0.55701       0.89056       0.92793       0.81813       0.80697       0.82037       0.8435        0.71352       \n","AGM(Adjusted geometric mean)                                      0.84267       0.89864       0.85284       0.94018       0.88995       0.8526        0.9877        0.92458       0.96391       0.72873       0.93258       0.89978       0.87223       0.9062        0.937         0.99877       0.83564       0.9456        0.81258       0.93001       0.9456        0.9712        0.94946       0.97413       0.71537       0.95065       0.96264       0.99878       0.89119       0.96759       0.97413       0.93817       0.97169       0.95184       0.99878       0.92206       0.88298       0.99878       0.89338       1.0           0.88412       0.93968       0.95436       0.99755       0.87063       0.86698       0.93582       0.96032       0.9669        0.79422       0.91071       0.96931       0.83689       0.87063       0.78136       0.96151       0.96366       0.89681       0.90158       0.94674       0.90758       0.84632       \n","AM(Difference between automatic and manual classification)        5             6             -3            -1            -4            -4            10            -2            2             -2            -2            5             -6            -3            1             1             -6            -1            -4            -1            -1            -1            0             -1            -2            -1            0             1             0             2             -1            0             3             -2            1             7             -1            1             0             0             -2            4             6             2             -3            -4            2             1             5             -11           3             3             -6            -3            2             4             1             -3            1             7             -3            -2            \n","AUC(Area under the ROC curve)                                     0.74265       0.82682       0.75          0.89121       0.80687       0.75          0.99179       0.86282       0.93505       0.60702       0.875         0.82763       0.78042       0.83251       0.88643       0.99918       0.72727       0.89918       0.69836       0.87336       0.89918       0.94444       0.90745       0.95          0.59508       0.90827       0.93168       0.99918       0.81005       0.94199       0.95          0.88725       0.95126       0.90909       0.99918       0.86765       0.79754       0.99918       0.8149        1.0           0.79836       0.89508       0.92286       0.99837       0.77696       0.77191       0.88562       0.92694       0.94508       0.67647       0.84508       0.94672       0.72995       0.77696       0.66178       0.93342       0.93502       0.81736       0.82922       0.91015       0.83333       0.74587       \n","AUCI(AUC value interpretation)                                    Good          Very Good     Good          Very Good     Very Good     Good          Excellent     Very Good     Excellent     Fair          Very Good     Very Good     Good          Very Good     Very Good     Excellent     Good          Very Good     Fair          Very Good     Very Good     Excellent     Excellent     Excellent     Poor          Excellent     Excellent     Excellent     Very Good     Excellent     Excellent     Very Good     Excellent     Excellent     Excellent     Very Good     Good          Excellent     Very Good     Excellent     Good          Very Good     Excellent     Excellent     Good          Good          Very Good     Excellent     Excellent     Fair          Very Good     Excellent     Good          Good          Fair          Excellent     Excellent     Very Good     Very Good     Excellent     Very Good     Good          \n","AUPR(Area under the PR curve)                                     0.40385       0.5           0.75          0.81593       0.75214       0.75          0.7619        0.80808       0.7875        0.25397       0.875         0.51515       0.73125       0.77778       0.73889       0.95455       0.72727       0.84444       0.53333       0.78409       0.84444       0.94444       0.81818       0.95          0.225         0.85909       0.86667       0.94444       0.625         0.80808       0.95          0.77778       0.81169       0.90909       0.9375        0.575         0.63333       0.94444       0.63636       1.0           0.675         0.68571       0.65934       0.9           0.69444       0.7013        0.70707       0.80357       0.75          0.67647       0.61923       0.79615       0.65934       0.69444       0.29167       0.72917       0.84926       0.75568       0.64103       0.60897       0.83333       0.54167       \n","BCD(Bray-Curtis dissimilarity)                                    0.00403       0.00484       0.00242       0.00081       0.00323       0.00323       0.00806       0.00161       0.00161       0.00161       0.00161       0.00403       0.00484       0.00242       0.00081       0.00081       0.00484       0.00081       0.00323       0.00081       0.00081       0.00081       0.0           0.00081       0.00161       0.00081       0.0           0.00081       0.0           0.00161       0.00081       0.0           0.00242       0.00161       0.00081       0.00565       0.00081       0.00081       0.0           0.0           0.00161       0.00323       0.00484       0.00161       0.00242       0.00323       0.00161       0.00081       0.00403       0.00887       0.00242       0.00242       0.00484       0.00242       0.00161       0.00323       0.00081       0.00242       0.00081       0.00565       0.00242       0.00161       \n","BM(Informedness or bookmaker informedness)                        0.48529       0.65364       0.5           0.78241       0.61374       0.5           0.98358       0.72563       0.8701        0.21404       0.75          0.65527       0.56084       0.66502       0.77287       0.99836       0.45455       0.79836       0.39672       0.74671       0.79836       0.88889       0.8149        0.9           0.19016       0.81654       0.86336       0.99837       0.6201        0.88398       0.9           0.7745        0.90252       0.81818       0.99837       0.73529       0.59508       0.99837       0.6298        1.0           0.59672       0.79016       0.84572       0.99673       0.55392       0.54381       0.77123       0.85388       0.89016       0.35294       0.69016       0.89344       0.45989       0.55392       0.32356       0.86683       0.87003       0.63472       0.65844       0.8203        0.66667       0.49175       \n","CEN(Confusion entropy)                                            0.25309       0.26408       0.12039       0.11636       0.13118       0.14837       0.18937       0.12472       0.1337        0.39678       0.07848       0.2613        0.1559        0.13715       0.16129       0.03018       0.14428       0.08159       0.24569       0.12934       0.08159       0.03469       0.10387       0.03226       0.2674        0.09053       0.0944        0.03469       0.19839       0.12472       0.03226       0.11767       0.10657       0.04793       0.03758       0.25723       0.22581       0.03469       0.20279       0             0.20055       0.19193       0.23501       0.06685       0.16866       0.18452       0.17265       0.11274       0.0981        0.21486       0.2554        0.12934       0.23501       0.16866       0.37178       0.17265       0.10153       0.1461        0.20223       0.25995       0.11274       0.27553       \n","DOR(Diagnostic odds ratio)                                        67.0          151.5         None          1107.33333    969.6         None          None          1621.33333    1421.0        34.62857      None          173.42857     775.28571     1214.0        709.33333     None          None          2436.0        202.66667     909.0         2436.0        None          1365.75       None          25.16667      2736.0        1959.75       None          338.33333     1621.33333    None          1065.75       1512.5        None          None          201.0         303.5         None          264.6875      None          456.0         402.66667     519.42857     None          762.5         729.6         531.125       1833.0        906.0         None          234.88889     1363.5        519.42857     762.5         50.66667      849.8         1402.33333    1064.0        241.2         378.75        None          120.2         \n","DP(Discriminant power)                                            1.00677       1.20212       None          1.6784        1.64659       None          None          1.7697        1.73812       0.84873       None          1.23449       1.59304       1.70042       1.57176       None          None          1.86717       1.2718        1.63114       1.86717       None          1.72862       None          0.77232       1.89498       1.81509       None          1.3945        1.7697        None          1.66923       1.75306       None          None          1.26982       1.36849       None          1.33572       None          1.46596       1.43618       1.49715       None          1.58906       1.5785        1.50248       1.79908       1.63035       None          1.30712       1.72823       1.49715       1.58906       0.93986       1.61502       1.73495       1.66884       1.31347       1.42152       None          1.14671       \n","DPI(Discriminant power interpretation)                            Limited       Limited       None          Limited       Limited       None          None          Limited       Limited       Poor          None          Limited       Limited       Limited       Limited       None          None          Limited       Limited       Limited       Limited       None          Limited       None          Poor          Limited       Limited       None          Limited       Limited       None          Limited       Limited       None          None          Limited       Limited       None          Limited       None          Limited       Limited       Limited       None          Limited       Limited       Limited       Limited       Limited       None          Limited       Limited       Limited       Limited       Poor          Limited       Limited       Limited       Limited       Limited       None          Limited       \n","ERR(Error rate)                                                   0.02097       0.01613       0.00484       0.00806       0.00968       0.00645       0.01613       0.00645       0.00645       0.01935       0.00323       0.01452       0.0129        0.00806       0.00806       0.00161       0.00968       0.00484       0.0129        0.00806       0.00484       0.00161       0.00645       0.00161       0.02258       0.00484       0.00645       0.00161       0.00968       0.00645       0.00161       0.00645       0.00806       0.00323       0.00161       0.01774       0.01129       0.00161       0.0129        0.0           0.00968       0.0129        0.0129        0.00323       0.00806       0.00968       0.00968       0.00484       0.01129       0.01774       0.01452       0.00806       0.0129        0.00806       0.01613       0.00968       0.00806       0.00806       0.01452       0.01452       0.00484       0.01935       \n","F0.5(F0.5 score)                                                  0.33333       0.37037       0.83333       0.83333       0.81633       0.83333       0.57895       0.85106       0.72917       0.27027       0.9375        0.4           0.80357       0.83333       0.71429       0.92593       0.80645       0.86957       0.58824       0.80357       0.86957       0.97561       0.81818       0.97826       0.2381        0.88235       0.86667       0.90909       0.625         0.75472       0.97826       0.77778       0.74627       0.95745       0.89744       0.44118       0.65217       0.90909       0.63636       1.0           0.71429       0.60606       0.50847       0.83333       0.75758       0.76923       0.66038       0.76923       0.64286       0.73171       0.56452       0.72581       0.73171       0.75758       0.26316       0.625         0.83333       0.81395       0.625         0.43103       0.90909       0.56452       \n","F1(F1 score - harmonic mean of precision and sensitivity)         0.38095       0.44444       0.66667       0.81481       0.72727       0.66667       0.6875        0.8           0.77778       0.25          0.85714       0.47059       0.69231       0.7619        0.73684       0.95238       0.625         0.84211       0.5           0.78261       0.84211       0.94118       0.81818       0.94737       0.22222       0.85714       0.86667       0.94118       0.625         0.8           0.94737       0.77778       0.8           0.9           0.93333       0.52174       0.63158       0.94118       0.63636       1.0           0.66667       0.66667       0.6           0.88889       0.66667       0.66667       0.7           0.8           0.72          0.52174       0.6087        0.78261       0.6           0.66667       0.28571       0.7           0.84848       0.73684       0.64          0.52632       0.8           0.53846       \n","F2(F2 score)                                                      0.44444       0.55556       0.55556       0.7971        0.65574       0.55556       0.84615       0.75472       0.83333       0.23256       0.78947       0.57143       0.60811       0.70175       0.76087       0.98039       0.5102        0.81633       0.43478       0.76271       0.81633       0.90909       0.81818       0.91837       0.20833       0.83333       0.86667       0.97561       0.625         0.85106       0.91837       0.77778       0.86207       0.84906       0.97222       0.6383        0.61224       0.97561       0.63636       1.0           0.625         0.74074       0.73171       0.95238       0.59524       0.58824       0.74468       0.83333       0.81818       0.40541       0.66038       0.84906       0.50847       0.59524       0.3125        0.79545       0.8642        0.67308       0.65574       0.67568       0.71429       0.51471       \n","FDR(False discovery rate)                                         0.69231       0.66667       0.0           0.15385       0.11111       0.0           0.47619       0.11111       0.3           0.71429       0.0           0.63636       0.1           0.11111       0.3           0.09091       0.0           0.11111       0.33333       0.18182       0.11111       0.0           0.18182       0.0           0.75          0.1           0.13333       0.11111       0.375         0.27273       0.0           0.22222       0.28571       0.0           0.125         0.6           0.33333       0.11111       0.36364       0.0           0.25          0.42857       0.53846       0.2           0.16667       0.14286       0.36364       0.25          0.4           0.0           0.46154       0.30769       0.14286       0.16667       0.75          0.41667       0.17647       0.125         0.38462       0.61538       0.0           0.41667       \n","FN(False negative/miss/type 2 error)                              4             2             3             3             5             4             0             3             1             7             2             2             7             4             2             0             6             2             6             3             2             1             2             1             8             2             2             0             3             1             1             2             1             2             0             2             4             0             4             0             4             2             1             0             4             5             2             1             1             11            3             1             7             4             4             1             2             4             4             1             3             7             \n","FNR(Miss rate or false negative rate)                             0.5           0.33333       0.5           0.21429       0.38462       0.5           0.0           0.27273       0.125         0.77778       0.25          0.33333       0.4375        0.33333       0.22222       0.0           0.54545       0.2           0.6           0.25          0.2           0.11111       0.18182       0.1           0.8           0.18182       0.13333       0.0           0.375         0.11111       0.1           0.22222       0.09091       0.18182       0.0           0.25          0.4           0.0           0.36364       0.0           0.4           0.2           0.14286       0.0           0.44444       0.45455       0.22222       0.14286       0.1           0.64706       0.3           0.1           0.53846       0.44444       0.66667       0.125         0.125         0.36364       0.33333       0.16667       0.33333       0.5           \n","FOR(False omission rate)                                          0.00659       0.00329       0.00486       0.00494       0.00818       0.00649       0.0           0.00491       0.00164       0.01142       0.00326       0.00328       0.01148       0.00655       0.00328       0.0           0.00976       0.00327       0.00977       0.00493       0.00327       0.00163       0.00328       0.00164       0.01307       0.00328       0.00331       0.0           0.0049        0.00164       0.00164       0.00327       0.00165       0.00327       0.0           0.00331       0.00655       0.0           0.00657       0.0           0.00654       0.0033        0.00165       0.0           0.00651       0.00816       0.00328       0.00163       0.00165       0.01792       0.00494       0.00165       0.01142       0.00651       0.00654       0.00164       0.00332       0.00654       0.00659       0.00165       0.00489       0.01151       \n","FP(False positive/type 1 error/false alarm)                       9             8             0             2             1             0             10            1             3             5             0             7             1             1             3             1             0             1             2             2             1             0             2             0             6             1             2             1             3             3             0             2             4             0             1             9             3             1             4             0             2             6             7             2             1             1             4             2             6             0             6             4             1             1             6             5             3             1             5             8             0             5             \n","FPR(Fall-out or false positive rate)                              0.01471       0.01303       0.0           0.0033        0.00165       0.0           0.01642       0.00164       0.0049        0.00818       0.0           0.0114        0.00166       0.00164       0.00491       0.00164       0.0           0.00164       0.00328       0.00329       0.00164       0.0           0.00328       0.0           0.00984       0.00164       0.00331       0.00163       0.0049        0.00491       0.0           0.00327       0.00657       0.0           0.00163       0.01471       0.00492       0.00163       0.00657       0.0           0.00328       0.00984       0.01142       0.00327       0.00164       0.00164       0.00655       0.00326       0.00984       0.0           0.00984       0.00656       0.00165       0.00164       0.00977       0.00817       0.00497       0.00164       0.00822       0.01303       0.0           0.00825       \n","G(G-measure geometric mean of precision and sensitivity)          0.39223       0.4714        0.70711       0.81537       0.7396        0.70711       0.72375       0.80403       0.78262       0.25198       0.86603       0.49237       0.71151       0.7698        0.73786       0.95346       0.6742        0.84327       0.5164        0.78335       0.84327       0.94281       0.81818       0.94868       0.22361       0.85812       0.86667       0.94281       0.625         0.80403       0.94868       0.77778       0.80582       0.90453       0.93541       0.54772       0.63246       0.94281       0.63636       1.0           0.67082       0.67612       0.62897       0.89443       0.68041       0.68376       0.70353       0.80178       0.73485       0.59409       0.61394       0.78935       0.62897       0.68041       0.28868       0.71443       0.84887       0.7462        0.64051       0.56614       0.8165        0.54006       \n","GI(Gini index)                                                    0.48529       0.65364       0.5           0.78241       0.61374       0.5           0.98358       0.72563       0.8701        0.21404       0.75          0.65527       0.56084       0.66502       0.77287       0.99836       0.45455       0.79836       0.39672       0.74671       0.79836       0.88889       0.8149        0.9           0.19016       0.81654       0.86336       0.99837       0.6201        0.88398       0.9           0.7745        0.90252       0.81818       0.99837       0.73529       0.59508       0.99837       0.6298        1.0           0.59672       0.79016       0.84572       0.99673       0.55392       0.54381       0.77123       0.85388       0.89016       0.35294       0.69016       0.89344       0.45989       0.55392       0.32356       0.86683       0.87003       0.63472       0.65844       0.8203        0.66667       0.49175       \n","GM(G-mean geometric mean of specificity and sensitivity)          0.70189       0.81116       0.70711       0.88494       0.78382       0.70711       0.99176       0.8521        0.93312       0.46947       0.86603       0.81183       0.74938       0.81582       0.87975       0.99918       0.6742        0.89369       0.63142       0.8646        0.89369       0.94281       0.90305       0.94868       0.44501       0.90379       0.92941       0.99918       0.78863       0.94049       0.94868       0.88047       0.95033       0.90453       0.99918       0.85963       0.77269       0.99918       0.7951        1.0           0.77333       0.89002       0.92052       0.99836       0.74475       0.73794       0.87903       0.92431       0.94401       0.59409       0.83254       0.94557       0.67881       0.74475       0.57452       0.93159       0.93309       0.79707       0.81313       0.9069        0.8165        0.70418       \n","IBA(Index of balanced accuracy)                                   0.25357       0.44723       0.25          0.61789       0.37909       0.25          0.99973       0.52925       0.76614       0.05078       0.5625        0.44689       0.31681       0.44481       0.60577       1.0           0.20661       0.64026       0.16078       0.56311       0.64026       0.79012       0.6699        0.81          0.04155       0.66966       0.75148       1.0           0.39176       0.79059       0.81          0.6055        0.82695       0.66942       1.0           0.5651        0.36117       1.0           0.40645       1.0           0.36078       0.6415        0.73598       0.99999       0.30904       0.29793       0.60604       0.73508       0.8108        0.12457       0.492         0.81055       0.21343       0.30904       0.11325       0.76646       0.76615       0.40534       0.44623       0.69611       0.44444       0.25203       \n","ICSI(Individual classification success index)                     -0.19231      0.0           0.5           0.63187       0.50427       0.5           0.52381       0.61616       0.575         -0.49206      0.75          0.0303        0.4625        0.55556       0.47778       0.90909       0.45455       0.68889       0.06667       0.56818       0.68889       0.88889       0.63636       0.9           -0.55         0.71818       0.73333       0.88889       0.25          0.61616       0.9           0.55556       0.62338       0.81818       0.875         0.15          0.26667       0.88889       0.27273       1.0           0.35          0.37143       0.31868       0.8           0.38889       0.4026        0.41414       0.60714       0.5           0.35294       0.23846       0.59231       0.31868       0.38889       -0.41667      0.45833       0.69853       0.51136       0.28205       0.21795       0.66667       0.08333       \n","IS(Information score)                                             4.57568       5.1062        6.69116       5.22776       5.40576       6.27612       4.88381       5.64677       5.76155       4.29884       6.27612       5.23173       5.12412       5.52124       5.59163       5.81669       5.81669       5.78427       5.36923       5.40166       5.78427       6.1062        5.52719       5.9542        3.9542        5.66469       5.16278       6.1062        5.59805       5.64677       5.9542        5.74363       5.33127       5.81669       6.27612       4.9542        5.36923       6.1062        5.16462       5.46877       5.53916       5.14684       5.35329       5.9542        5.84316       5.5943        5.45412       6.05373       5.21723       5.18866       5.06111       5.42368       5.35329       5.84316       4.69116       5.49852       4.99602       5.62405       4.99072       5.31265       6.1062        4.69116       \n","J(Jaccard index)                                                  0.23529       0.28571       0.5           0.6875        0.57143       0.5           0.52381       0.66667       0.63636       0.14286       0.75          0.30769       0.52941       0.61538       0.58333       0.90909       0.45455       0.72727       0.33333       0.64286       0.72727       0.88889       0.69231       0.9           0.125         0.75          0.76471       0.88889       0.45455       0.66667       0.9           0.63636       0.66667       0.81818       0.875         0.35294       0.46154       0.88889       0.46667       1.0           0.5           0.5           0.42857       0.8           0.5           0.5           0.53846       0.66667       0.5625        0.35294       0.4375        0.64286       0.42857       0.5           0.16667       0.53846       0.73684       0.58333       0.47059       0.35714       0.66667       0.36842       \n","LS(Lift score)                                                    23.84615      34.44444      103.33333     37.47253      42.39316      77.5          29.52381      50.10101      54.25         19.68254      77.5          37.57576      34.875        45.92593      48.22222      56.36364      56.36364      55.11111      41.33333      42.27273      55.11111      68.88889      46.1157       62.0          15.5          50.72727      35.82222      68.88889      48.4375       50.10101      62.0          53.58025      40.25974      56.36364      77.5          31.0          41.33333      68.88889      35.86777      44.28571      46.5          35.42857      40.87912      62.0          57.40741      48.31169      43.83838      66.42857      37.2          36.47059      33.38462      42.92308      40.87912      57.40741      25.83333      45.20833      31.91176      49.31818      31.79487      39.74359      68.88889      25.83333      \n","MCC(Matthews correlation coefficient)                             0.38226       0.46447       0.70539       0.81128       0.7352        0.70481       0.71778       0.8009        0.77951       0.2423        0.86461       0.48593       0.70592       0.76601       0.73381       0.95268       0.6709        0.84086       0.51049       0.77927       0.84086       0.94204       0.8149        0.94791       0.21226       0.85569       0.86336       0.94204       0.6201        0.8009        0.94791       0.7745        0.80198       0.90305       0.93465       0.54008       0.62676       0.94204       0.6298        1.0           0.66606       0.67001       0.62365       0.89296       0.67675       0.67948       0.69875       0.79938       0.72981       0.58874       0.60681       0.78554       0.62365       0.67675       0.28067       0.71009       0.84476       0.74245       0.63313       0.56049       0.8145        0.53028       \n","MCCI(Matthews correlation coefficient interpretation)             Weak          Weak          Strong        Strong        Strong        Strong        Strong        Strong        Strong        Negligible    Strong        Weak          Strong        Strong        Strong        Very Strong   Moderate      Strong        Moderate      Strong        Strong        Very Strong   Strong        Very Strong   Negligible    Strong        Strong        Very Strong   Moderate      Strong        Very Strong   Strong        Strong        Very Strong   Very Strong   Moderate      Moderate      Very Strong   Moderate      Very Strong   Moderate      Moderate      Moderate      Strong        Moderate      Moderate      Moderate      Strong        Strong        Moderate      Moderate      Strong        Moderate      Moderate      Negligible    Strong        Strong        Strong        Moderate      Moderate      Strong        Moderate      \n","MCEN(Modified confusion entropy)                                  0.279         0.30216       0.13839       0.16232       0.16582       0.18036       0.24682       0.17242       0.18151       0.42964       0.10821       0.30304       0.19682       0.18316       0.21552       0.04538       0.16733       0.1099        0.28767       0.17558       0.1099        0.05082       0.14208       0.04793       0.27938       0.12931       0.13877       0.05082       0.24603       0.17242       0.04793       0.15527       0.14216       0.06452       0.05411       0.3073        0.28749       0.05082       0.25491       0             0.25863       0.24569       0.2933        0.09586       0.21079       0.23458       0.22423       0.15246       0.11263       0.24997       0.32464       0.17558       0.2933        0.21079       0.407         0.22423       0.1461        0.19148       0.25489       0.31193       0.15246       0.33581       \n","MK(Markedness)                                                    0.3011        0.33004       0.99514       0.84121       0.88071       0.99351       0.52381       0.88398       0.69836       0.2743        0.99674       0.36035       0.88852       0.88234       0.69672       0.90909       0.99024       0.88562       0.65689       0.81326       0.88562       0.99837       0.8149        0.99836       0.23693       0.89672       0.86336       0.88889       0.6201        0.72563       0.99836       0.7745        0.71264       0.99673       0.875         0.39669       0.66012       0.88889       0.6298        1.0           0.74346       0.56813       0.45989       0.8           0.82682       0.84899       0.63308       0.74837       0.59835       0.98208       0.53352       0.69066       0.84572       0.82682       0.24346       0.58169       0.82021       0.86846       0.60879       0.38297       0.99511       0.57182       \n","N(Condition negative)                                             612           614           614           606           607           612           609           609           612           611           612           614           604           608           611           610           609           610           610           608           610           611           609           610           610           609           605           612           612           611           610           611           609           609           613           612           610           612           609           606           610           610           613           612           611           609           611           613           610           603           610           610           607           611           614           612           604           609           608           614           611           606           \n","NLR(Negative likelihood ratio)                                    0.50746       0.33773       0.5           0.215         0.38525       0.5           0.0           0.27318       0.12562       0.7842        0.25          0.33718       0.43823       0.33388       0.22332       0.0           0.54545       0.20033       0.60197       0.25083       0.20033       0.11111       0.18242       0.1           0.80795       0.18212       0.13378       0.0           0.37685       0.11166       0.1           0.22295       0.09151       0.18182       0.0           0.25373       0.40198       0.0           0.36604       0.0           0.40132       0.20199       0.14451       0.0           0.44517       0.45529       0.22369       0.14332       0.10099       0.64706       0.30298       0.10066       0.53935       0.44517       0.67325       0.12603       0.12562       0.36423       0.3361        0.16887       0.33333       0.50416       \n","NLRI(Negative likelihood ratio interpretation)                    Negligible    Poor          Negligible    Poor          Poor          Negligible    Good          Poor          Fair          Negligible    Poor          Poor          Poor          Poor          Poor          Good          Negligible    Poor          Negligible    Poor          Poor          Fair          Fair          Good          Negligible    Fair          Fair          Good          Poor          Fair          Good          Poor          Good          Fair          Good          Poor          Poor          Good          Poor          Good          Poor          Poor          Fair          Good          Poor          Poor          Poor          Fair          Fair          Negligible    Poor          Fair          Negligible    Poor          Negligible    Fair          Fair          Poor          Poor          Fair          Poor          Negligible    \n","NPV(Negative predictive value)                                    0.99341       0.99671       0.99514       0.99506       0.99182       0.99351       1.0           0.99509       0.99836       0.98858       0.99674       0.99672       0.98852       0.99345       0.99672       1.0           0.99024       0.99673       0.99023       0.99507       0.99673       0.99837       0.99672       0.99836       0.98693       0.99672       0.99669       1.0           0.9951        0.99836       0.99836       0.99673       0.99835       0.99673       1.0           0.99669       0.99345       1.0           0.99343       1.0           0.99346       0.9967        0.99835       1.0           0.99349       0.99184       0.99672       0.99837       0.99835       0.98208       0.99506       0.99835       0.98858       0.99349       0.99346       0.99836       0.99668       0.99346       0.99341       0.99835       0.99511       0.98849       \n","OC(Overlap coefficient)                                           0.5           0.66667       1.0           0.84615       0.88889       1.0           1.0           0.88889       0.875         0.28571       1.0           0.66667       0.9           0.88889       0.77778       1.0           1.0           0.88889       0.66667       0.81818       0.88889       1.0           0.81818       1.0           0.25          0.9           0.86667       1.0           0.625         0.88889       1.0           0.77778       0.90909       1.0           1.0           0.75          0.66667       1.0           0.63636       1.0           0.75          0.8           0.85714       1.0           0.83333       0.85714       0.77778       0.85714       0.9           1.0           0.7           0.9           0.85714       0.83333       0.33333       0.875         0.875         0.875         0.66667       0.83333       1.0           0.58333       \n","OOC(Otsuka-Ochiai coefficient)                                    0.39223       0.4714        0.70711       0.81537       0.7396        0.70711       0.72375       0.80403       0.78262       0.25198       0.86603       0.49237       0.71151       0.7698        0.73786       0.95346       0.6742        0.84327       0.5164        0.78335       0.84327       0.94281       0.81818       0.94868       0.22361       0.85812       0.86667       0.94281       0.625         0.80403       0.94868       0.77778       0.80582       0.90453       0.93541       0.54772       0.63246       0.94281       0.63636       1.0           0.67082       0.67612       0.62897       0.89443       0.68041       0.68376       0.70353       0.80178       0.73485       0.59409       0.61394       0.78935       0.62897       0.68041       0.28868       0.71443       0.84887       0.7462        0.64051       0.56614       0.8165        0.54006       \n","OP(Optimized precision)                                           0.6523        0.79017       0.66183       0.87356       0.75301       0.66022       0.97559       0.83645       0.92933       0.34673       0.85392       0.79099       0.70786       0.79273       0.86936       0.99757       0.61532       0.88486       0.55987       0.85069       0.88486       0.93956       0.89518       0.94576       0.31351       0.89597       0.92377       0.99757       0.76188       0.93718       0.94576       0.87016       0.9476        0.89677       0.99757       0.84666       0.74102       0.99757       0.76801       1.0           0.74186       0.88087       0.91588       0.99514       0.70697       0.69696       0.86856       0.91986       0.94101       0.504         0.81381       0.94258       0.61939       0.70697       0.48756       0.92774       0.92775       0.77049       0.78945       0.90108       0.79516       0.651         \n","P(Condition positive or support)                                  8             6             6             14            13            8             11            11            8             9             8             6             16            12            9             10            11            10            10            12            10            9             11            10            10            11            15            8             8             9             10            9             11            11            7             8             10            8             11            14            10            10            7             8             9             11            9             7             10            17            10            10            13            9             6             8             16            11            12            6             9             14            \n","PLR(Positive likelihood ratio)                                    34.0          51.16667      None          238.07143     373.53846     None          60.9          442.90909     178.5         27.15556      None          58.47619      339.75        405.33333     158.40741     610.0         None          488.0         122.0         228.0         488.0         None          249.13636     None          20.33333      498.27273     262.16667     612.0         127.5         181.03704     None          237.61111     138.40909     None          613.0         51.0          122.0         612.0         96.88636      None          183.0         81.33333      75.06122      306.0         339.44444     332.18182     118.80556     262.71429     91.5          None          71.16667      137.25        280.15385     339.44444     34.11111      107.1         176.16667     387.54545     81.06667      63.95833      None          60.6          \n","PLRI(Positive likelihood ratio interpretation)                    Good          Good          None          Good          Good          None          Good          Good          Good          Good          None          Good          Good          Good          Good          Good          None          Good          Good          Good          Good          None          Good          None          Good          Good          Good          Good          Good          Good          None          Good          Good          None          Good          Good          Good          Good          Good          None          Good          Good          Good          Good          Good          Good          Good          Good          Good          None          Good          Good          Good          Good          Good          Good          Good          Good          Good          Good          None          Good          \n","POP(Population)                                                   620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           620           \n","PPV(Precision or positive predictive value)                       0.30769       0.33333       1.0           0.84615       0.88889       1.0           0.52381       0.88889       0.7           0.28571       1.0           0.36364       0.9           0.88889       0.7           0.90909       1.0           0.88889       0.66667       0.81818       0.88889       1.0           0.81818       1.0           0.25          0.9           0.86667       0.88889       0.625         0.72727       1.0           0.77778       0.71429       1.0           0.875         0.4           0.66667       0.88889       0.63636       1.0           0.75          0.57143       0.46154       0.8           0.83333       0.85714       0.63636       0.75          0.6           1.0           0.53846       0.69231       0.85714       0.83333       0.25          0.58333       0.82353       0.875         0.61538       0.38462       1.0           0.58333       \n","PRE(Prevalence)                                                   0.0129        0.00968       0.00968       0.02258       0.02097       0.0129        0.01774       0.01774       0.0129        0.01452       0.0129        0.00968       0.02581       0.01935       0.01452       0.01613       0.01774       0.01613       0.01613       0.01935       0.01613       0.01452       0.01774       0.01613       0.01613       0.01774       0.02419       0.0129        0.0129        0.01452       0.01613       0.01452       0.01774       0.01774       0.01129       0.0129        0.01613       0.0129        0.01774       0.02258       0.01613       0.01613       0.01129       0.0129        0.01452       0.01774       0.01452       0.01129       0.01613       0.02742       0.01613       0.01613       0.02097       0.01452       0.00968       0.0129        0.02581       0.01774       0.01935       0.00968       0.01452       0.02258       \n","Q(Yule Q - coefficient of colligation)                            0.97059       0.98689       None          0.9982        0.99794       None          None          0.99877       0.99859       0.94387       None          0.98853       0.99742       0.99835       0.99718       None          None          0.99918       0.99018       0.9978        0.99918       None          0.99854       None          0.92357       0.99927       0.99898       None          0.99411       0.99877       None          0.99813       0.99868       None          None          0.9901        0.99343       None          0.99247       None          0.99562       0.99505       0.99616       None          0.99738       0.99726       0.99624       0.99891       0.99779       None          0.99152       0.99853       0.99616       0.99738       0.96129       0.99765       0.99857       0.99812       0.99174       0.99473       None          0.9835        \n","QI(Yule Q interpretation)                                         Strong        Strong        None          Strong        Strong        None          None          Strong        Strong        Strong        None          Strong        Strong        Strong        Strong        None          None          Strong        Strong        Strong        Strong        None          Strong        None          Strong        Strong        Strong        None          Strong        Strong        None          Strong        Strong        None          None          Strong        Strong        None          Strong        None          Strong        Strong        Strong        None          Strong        Strong        Strong        Strong        Strong        None          Strong        Strong        Strong        Strong        Strong        Strong        Strong        Strong        Strong        Strong        None          Strong        \n","RACC(Random accuracy)                                             0.00027       0.00019       5e-05         0.00047       0.0003        8e-05         0.0006        0.00026       0.00021       0.00016       0.00012       0.00017       0.00042       0.00028       0.00023       0.00029       0.00014       0.00023       0.00016       0.00034       0.00023       0.00019       0.00031       0.00023       0.00021       0.00029       0.00059       0.00019       0.00017       0.00026       0.00023       0.00021       0.0004        0.00026       0.00015       0.00031       0.00023       0.00019       0.00031       0.00051       0.00021       0.00036       0.00024       0.00021       0.00014       0.0002        0.00026       0.00015       0.00039       0.00027       0.00034       0.00034       0.00024       0.00014       0.00012       0.00025       0.00071       0.00023       0.00041       0.0002        0.00014       0.00044       \n","RACCU(Random accuracy unbiased)                                   0.00029       0.00021       5e-05         0.00047       0.00031       9e-05         0.00067       0.00026       0.00021       0.00017       0.00013       0.00019       0.00044       0.00029       0.00023       0.00029       0.00017       0.00023       0.00017       0.00034       0.00023       0.00019       0.00031       0.00023       0.00021       0.00029       0.00059       0.00019       0.00017       0.00026       0.00023       0.00021       0.00041       0.00026       0.00015       0.00034       0.00023       0.00019       0.00031       0.00051       0.00021       0.00037       0.00026       0.00021       0.00015       0.00021       0.00026       0.00015       0.00041       0.00034       0.00034       0.00034       0.00026       0.00015       0.00013       0.00026       0.00071       0.00023       0.00041       0.00023       0.00015       0.00044       \n","TN(True negative/correct rejection)                               603           606           614           604           606           612           599           608           609           606           612           607           603           607           608           609           609           609           608           606           609           611           607           610           604           608           603           611           609           608           610           609           605           609           612           603           607           611           605           606           608           604           606           610           610           608           607           611           604           603           604           606           606           610           608           607           601           608           603           606           611           601           \n","TNR(Specificity or true negative rate)                            0.98529       0.98697       1.0           0.9967        0.99835       1.0           0.98358       0.99836       0.9951        0.99182       1.0           0.9886        0.99834       0.99836       0.99509       0.99836       1.0           0.99836       0.99672       0.99671       0.99836       1.0           0.99672       1.0           0.99016       0.99836       0.99669       0.99837       0.9951        0.99509       1.0           0.99673       0.99343       1.0           0.99837       0.98529       0.99508       0.99837       0.99343       1.0           0.99672       0.99016       0.98858       0.99673       0.99836       0.99836       0.99345       0.99674       0.99016       1.0           0.99016       0.99344       0.99835       0.99836       0.99023       0.99183       0.99503       0.99836       0.99178       0.98697       1.0           0.99175       \n","TON(Test outcome negative)                                        607           608           617           607           611           616           599           611           610           613           614           609           610           611           610           609           615           611           614           609           611           612           609           611           612           610           605           611           612           609           611           611           606           611           612           605           611           611           609           606           612           606           607           610           614           613           609           612           605           614           607           607           613           614           612           608           603           612           607           607           614           608           \n","TOP(Test outcome positive)                                        13            12            3             13            9             4             21            9             10            7             6             11            10            9             10            11            5             9             6             11            9             8             11            9             8             10            15            9             8             11            9             9             14            9             8             15            9             9             11            14            8             14            13            10            6             7             11            8             15            6             13            13            7             6             8             12            17            8             13            13            6             12            \n","TP(True positive/hit)                                             4             4             3             11            8             4             11            8             7             2             6             4             9             8             7             10            5             8             4             9             8             8             9             9             2             9             13            8             5             8             9             7             10            9             7             6             6             8             7             14            6             8             6             8             5             6             7             6             9             6             7             9             6             5             2             7             14            7             8             5             6             7             \n","TPR(Sensitivity, recall, hit rate, or true positive rate)         0.5           0.66667       0.5           0.78571       0.61538       0.5           1.0           0.72727       0.875         0.22222       0.75          0.66667       0.5625        0.66667       0.77778       1.0           0.45455       0.8           0.4           0.75          0.8           0.88889       0.81818       0.9           0.2           0.81818       0.86667       1.0           0.625         0.88889       0.9           0.77778       0.90909       0.81818       1.0           0.75          0.6           1.0           0.63636       1.0           0.6           0.8           0.85714       1.0           0.55556       0.54545       0.77778       0.85714       0.9           0.35294       0.7           0.9           0.46154       0.55556       0.33333       0.875         0.875         0.63636       0.66667       0.83333       0.66667       0.5           \n","Y(Youden index)                                                   0.48529       0.65364       0.5           0.78241       0.61374       0.5           0.98358       0.72563       0.8701        0.21404       0.75          0.65527       0.56084       0.66502       0.77287       0.99836       0.45455       0.79836       0.39672       0.74671       0.79836       0.88889       0.8149        0.9           0.19016       0.81654       0.86336       0.99837       0.6201        0.88398       0.9           0.7745        0.90252       0.81818       0.99837       0.73529       0.59508       0.99837       0.6298        1.0           0.59672       0.79016       0.84572       0.99673       0.55392       0.54381       0.77123       0.85388       0.89016       0.35294       0.69016       0.89344       0.45989       0.55392       0.32356       0.86683       0.87003       0.63472       0.65844       0.8203        0.66667       0.49175       \n","dInd(Distance index)                                              0.50022       0.33359       0.5           0.21431       0.38462       0.5           0.01642       0.27273       0.1251        0.77782       0.25          0.33353       0.4375        0.33334       0.22228       0.00164       0.54545       0.20001       0.60001       0.25002       0.20001       0.11111       0.18185       0.1           0.80006       0.18183       0.13337       0.00163       0.37503       0.11122       0.1           0.22225       0.09115       0.18182       0.00163       0.25043       0.40003       0.00163       0.3637        0.0           0.40001       0.20024       0.14331       0.00327       0.44445       0.45455       0.22232       0.14289       0.10048       0.64706       0.30016       0.10021       0.53846       0.44445       0.66674       0.12527       0.1251        0.36364       0.33343       0.16718       0.33333       0.50007       \n","sInd(Similarity index)                                            0.64629       0.76412       0.64645       0.84846       0.72803       0.64645       0.98839       0.80715       0.91154       0.45          0.82322       0.76416       0.69064       0.76429       0.84283       0.99884       0.61431       0.85857       0.57573       0.82321       0.85857       0.92143       0.87141       0.92929       0.43427       0.87143       0.90569       0.99884       0.73481       0.92136       0.92929       0.84285       0.93555       0.87144       0.99885       0.82292       0.71714       0.99884       0.74283       1.0           0.71715       0.85841       0.89866       0.99769       0.68573       0.67859       0.8428        0.89896       0.92895       0.54246       0.78775       0.92914       0.61925       0.68573       0.52854       0.91142       0.91154       0.74287       0.76423       0.88179       0.7643        0.6464        \n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pycm/pycm_obj.py:195: RuntimeWarning: The confusion matrix is a high dimension matrix and won't be demonstrated properly.\n","If confusion matrix has too many zeros (sparse matrix) you can set `sparse` flag to True in printing functions otherwise by using save_csv method to save the confusion matrix in csv format you'll have better demonstration.\n","  warn(CLASS_NUMBER_WARNING, RuntimeWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HBH_FkIQc_n_"},"source":[""],"execution_count":null,"outputs":[]}]}